# -*- coding: utf-8 -*-
"""5 models_Instructvalidation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZgTzjzbv9V2x72yXZBoHMvrSH-UaQKIK
"""

# Commented out IPython magic to ensure Python compatibility.
"""
EXPERIMENT 4: Microsoft Validation
===================================
Goal: Verify that refusal directions actually separate harmful/harmless prompts

Steps:
1. Load harmful and harmless datasets
2. Compute direction at each layer: harmful_mean - harmless_mean
3. Test statistical separation (t-test, Cohen's d)
4. Identify valid layers
"""

# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Navigate
# %cd /content/drive/MyDrive/LLM-refusal-directions-and-preferences

# Add to path
import sys
sys.path.append('/content/drive/MyDrive/LLM-refusal-directions-and-preferences')

print("✓ Setup complete")

# Install requirements
!pip install -q -r requirements.txt

# Imports
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from scipy.stats import ttest_ind
import matplotlib.pyplot as plt
from tqdm import tqdm

print("✓ All imports successful")
print(f"✓ CUDA available: {torch.cuda.is_available()}")

# Load model
model_name = "microsoft/Phi-3.5-mini-instruct"
print(f"Loading {model_name}...")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

n_layers = len(model.model.layers)
hidden_size = model.config.hidden_size

print(f"✓ Model loaded")
print(f"✓ Layers: {n_layers}")
print(f"✓ Hidden size: {hidden_size}")

# Commented out IPython magic to ensure Python compatibility.
"""
EXPERIMENT 1: Model Family Comparison - mlabonne harmful_behaviors
==================================================================
Goal: Rank harmful prompts across different model families
Steps:
1. Load harmful prompts dataset (mlabonne/harmful_behaviors)
2. For each model: compute refusal direction, rank all harmful prompts
3. Compare rankings across models
"""

# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Navigate
# %cd /content/drive/MyDrive/LLM-refusal-directions-and-preferences

# Add to path
import sys
sys.path.append('/content/drive/MyDrive/LLM-refusal-directions-and-preferences')

# Imports
import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from scipy.stats import ttest_ind
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import pickle
import json
import os

print("✓ All imports successful")
print(f"✓ CUDA available: {torch.cuda.is_available()}")

# ============================================================================
# CONFIGURATION
# ============================================================================

MODELS_TO_TEST = {
    # Small models (~1-2B)
    'qwen_1.5b': 'Qwen/Qwen2.5-1.5B-Instruct',
    'llama_1b': 'meta-llama/Llama-3.2-1B-Instruct',
    'gemma_2b': 'google/gemma-2-2b-it',

    # Medium models (~3B)
    'llama_3b': 'meta-llama/Llama-3.2-3B-Instruct',
    'phi_3.5': 'microsoft/Phi-3.5-mini-instruct',

    # Large models (~7-8B)
    'llama_8b': 'meta-llama/Llama-3.1-8B-Instruct',
    'qwen_7b': 'Qwen/Qwen2.5-7B-Instruct',
}

SAVE_DIR = '/content/drive/MyDrive/LLM-refusal-directions-and-preferences/model_comparison'
os.makedirs(SAVE_DIR, exist_ok=True)

# Number of prompts to use
N_HARMFUL_TRAIN = 256  # For computing direction
N_HARMLESS_TRAIN = 256
N_HARMFUL_TEST = None  # None = use all available for ranking

print("\n" + "="*60)
print("CONFIGURATION")
print("="*60)
print(f"Models to analyze: {len(MODELS_TO_TEST)}")
print(f"Harmful prompts for direction: {N_HARMFUL_TRAIN}")
print(f"Harmless prompts for direction: {N_HARMLESS_TRAIN}")
print(f"Harmful prompts to rank: {'All' if N_HARMFUL_TEST is None else N_HARMFUL_TEST}")
print(f"Save directory: {SAVE_DIR}")

# ============================================================================
# LOAD DATASETS
# ============================================================================

print("\n" + "="*60)
print("LOADING DATASETS")
print("="*60)

# Load harmful behaviors
print("\nLoading harmful behaviors...")
harmful_ds = load_dataset('mlabonne/harmful_behaviors')
all_harmful_prompts = harmful_ds['train']['text']

print(f"✓ Total harmful prompts available: {len(all_harmful_prompts)}")

# Split into train (for direction) and test (for ranking)
harmful_train = all_harmful_prompts[:N_HARMFUL_TRAIN]
harmful_test = all_harmful_prompts if N_HARMFUL_TEST is None else all_harmful_prompts[:N_HARMFUL_TEST]

print(f"✓ Harmful prompts for direction computation: {len(harmful_train)}")
print(f"✓ Harmful prompts to rank: {len(harmful_test)}")

# Load harmless behaviors
print("\nLoading harmless behaviors...")
harmless_ds = load_dataset('mlabonne/harmless_alpaca')
harmless_train = harmless_ds['train']['text'][:N_HARMLESS_TRAIN]

print(f"✓ Harmless prompts for direction: {len(harmless_train)}")

# Show examples
print("\n" + "="*60)
print("DATASET EXAMPLES")
print("="*60)
print(f"\nHarmful example 1: {harmful_test[0][:120]}...")
print(f"Harmful example 2: {harmful_test[1][:120]}...")
print(f"\nHarmless example 1: {harmless_train[0][:120]}...")
print(f"Harmless example 2: {harmless_train[1][:120]}...")

# Create dataframe for test prompts
df_harmful_test = pd.DataFrame({
    'prompt_id': range(len(harmful_test)),
    'prompt': harmful_test
})

print(f"\n✓ Created test dataframe with {len(df_harmful_test)} prompts")

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_activation(prompt, model, tokenizer, layer_idx):
    """Extract activation at last token position for specific layer"""

    # Format prompt
    messages = [{"role": "user", "content": prompt}]
    formatted = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Tokenize
    inputs = tokenizer(
        formatted,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(model.device)

    # Get activations
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)

    # Extract last token position, specific layer
    last_pos = inputs['input_ids'].shape[1] - 1
    activation = outputs.hidden_states[layer_idx + 1][0, last_pos, :].cpu().numpy()

    return activation

def compute_refusal_direction(model, tokenizer, harmful_prompts, harmless_prompts):
    """Compute refusal direction for FINAL layer"""

    n_layers = len(model.model.layers)
    final_layer_idx = n_layers - 1

    print(f"\nComputing refusal direction at final layer ({final_layer_idx})...")

    # Collect activations
    harmful_acts = []
    harmless_acts = []

    # Harmful prompts
    for prompt in tqdm(harmful_prompts, desc="Processing harmful"):
        act = get_activation(prompt, model, tokenizer, final_layer_idx)
        harmful_acts.append(act)

    # Harmless prompts
    for prompt in tqdm(harmless_prompts, desc="Processing harmless"):
        act = get_activation(prompt, model, tokenizer, final_layer_idx)
        harmless_acts.append(act)

    # Compute direction: harmful_mean - harmless_mean
    harmful_mean = np.array(harmful_acts).mean(axis=0)
    harmless_mean = np.array(harmless_acts).mean(axis=0)
    direction = harmful_mean - harmless_mean

    # Normalize
    direction_norm = direction / np.linalg.norm(direction)
    magnitude = np.linalg.norm(direction)

    # Statistical validation
    harmful_projections = [np.dot(act, direction_norm) for act in harmful_acts]
    harmless_projections = [np.dot(act, direction_norm) for act in harmless_acts]

    t_stat, p_val = ttest_ind(harmful_projections, harmless_projections)

    mean_diff = np.mean(harmful_projections) - np.mean(harmless_projections)
    pooled_std = np.sqrt(
        (np.std(harmful_projections)**2 + np.std(harmless_projections)**2) / 2
    )
    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0

    print(f"\n✓ Refusal direction computed!")
    print(f"  Layer: {final_layer_idx}/{n_layers-1}")
    print(f"  Magnitude: {magnitude:.3f}")
    print(f"  Cohen's d: {cohens_d:.3f}")
    print(f"  p-value: {p_val:.2e}")
    print(f"  Valid: {'✓' if (p_val < 0.01 and abs(cohens_d) > 0.5) else '✗'}")

    return {
        'layer': final_layer_idx,
        'direction': direction_norm,
        'magnitude': magnitude,
        'cohens_d': cohens_d,
        'p_value': p_val,
        'harmful_mean_proj': np.mean(harmful_projections),
        'harmless_mean_proj': np.mean(harmless_projections),
        'separation': mean_diff
    }

def rank_harmful_prompts(model, tokenizer, prompts, direction_info):
    """Score and rank all harmful prompts"""

    direction = direction_info['direction']
    layer_idx = direction_info['layer']

    print(f"\nRanking {len(prompts)} harmful prompts...")

    # Compute refusal scores
    scores = []

    for prompt in tqdm(prompts, desc="Scoring prompts"):
        try:
            act = get_activation(prompt, model, tokenizer, layer_idx)
            score = np.dot(act, direction)
            scores.append(score)
        except Exception as e:
            print(f"\nError: {e}")
            scores.append(np.nan)

    # Create results dataframe
    df_results = pd.DataFrame({
        'prompt_id': range(len(prompts)),
        'prompt': prompts,
        'refusal_score': scores
    })

    # Remove NaN
    df_results = df_results.dropna(subset=['refusal_score'])

    # Add rank (1 = highest refusal)
    df_results['refusal_rank'] = df_results['refusal_score'].rank(ascending=False, method='min').astype(int)

    # Sort by rank
    df_results = df_results.sort_values('refusal_rank').reset_index(drop=True)

    print(f"✓ Ranked {len(df_results)} prompts")
    print(f"  Score range: [{df_results['refusal_score'].min():.4f}, {df_results['refusal_score'].max():.4f}]")
    print(f"  Mean score: {df_results['refusal_score'].mean():.4f}")
    print(f"  Std score: {df_results['refusal_score'].std():.4f}")

    return df_results

# ============================================================================
# MAIN ANALYSIS LOOP
# ============================================================================

print("\n" + "="*60)
print("ANALYZING MODELS")
print("="*60)

all_model_results = {}

for model_name, model_path in MODELS_TO_TEST.items():

    print("\n" + "="*70)
    print(f"MODEL: {model_name}")
    print("="*70)

    try:
        # 1. Load model
        print(f"\n[1/4] Loading model...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )

        n_layers = len(model.model.layers)
        hidden_size = model.config.hidden_size

        print(f"✓ Model loaded")
        print(f"  Architecture: {model.config.model_type}")
        print(f"  Layers: {n_layers}")
        print(f"  Hidden size: {hidden_size}")

        # 2. Compute refusal direction
        print(f"\n[2/4] Computing refusal direction...")
        direction_info = compute_refusal_direction(
            model, tokenizer, harmful_train, harmless_train
        )

        # 3. Rank all harmful prompts
        print(f"\n[3/4] Ranking harmful prompts...")
        df_rankings = rank_harmful_prompts(
            model, tokenizer, harmful_test, direction_info
        )

        # 4. Save results
        print(f"\n[4/4] Saving results...")

        # Store results
        results = {
            'model_name': model_name,
            'model_path': model_path,
            'n_layers': n_layers,
            'hidden_size': hidden_size,
            'direction_info': direction_info,
            'rankings': df_rankings
        }

        # Save pickle (complete results)
        pickle_path = f'{SAVE_DIR}/{model_name}_results.pkl'
        with open(pickle_path, 'wb') as f:
            pickle.dump(results, f)

        # Save CSV (rankings only)
        csv_path = f'{SAVE_DIR}/{model_name}_rankings.csv'
        df_rankings.to_csv(csv_path, index=False)

        print(f"✓ Saved to:")
        print(f"  {pickle_path}")
        print(f"  {csv_path}")

        # Show top 10
        print(f"\nTop 10 highest refusal scores:")
        for idx, row in df_rankings.head(10).iterrows():
            print(f"  Rank {row['refusal_rank']}: {row['refusal_score']:.4f} - {row['prompt'][:80]}...")

        all_model_results[model_name] = results

        # Cleanup
        del model
        torch.cuda.empty_cache()

        print(f"\n✓ {model_name} complete!")

    except Exception as e:
        print(f"\n✗ ERROR with {model_name}: {e}")
        import traceback
        traceback.print_exc()
        continue

# ============================================================================
# SAVE SUMMARY
# ============================================================================

print("\n" + "="*60)
print("ANALYSIS SUMMARY")
print("="*60)

print(f"\n✓ Successfully analyzed: {len(all_model_results)}/{len(MODELS_TO_TEST)} models")

# Create summary table
summary_data = []
for model_name, results in all_model_results.items():
    summary_data.append({
        'model': model_name,
        'n_layers': results['n_layers'],
        'hidden_size': results['hidden_size'],
        'cohens_d': results['direction_info']['cohens_d'],
        'p_value': results['direction_info']['p_value'],
        'magnitude': results['direction_info']['magnitude'],
        'separation': results['direction_info']['separation'],
        'n_prompts_ranked': len(results['rankings'])
    })

df_summary = pd.DataFrame(summary_data)
print("\nModel Summary:")
print(df_summary.to_string(index=False))

# Save summary
summary_path = f'{SAVE_DIR}/model_summary.csv'
df_summary.to_csv(summary_path, index=False)

summary_json_path = f'{SAVE_DIR}/analysis_summary.json'
with open(summary_json_path, 'w') as f:
    json.dump({
        'models_analyzed': list(all_model_results.keys()),
        'total_models_attempted': len(MODELS_TO_TEST),
        'n_harmful_test': len(harmful_test),
        'n_harmful_train': len(harmful_train),
        'n_harmless_train': len(harmless_train),
        'save_directory': SAVE_DIR
    }, f, indent=2)

print(f"\n✓ Summary saved to:")
print(f"  {summary_path}")
print(f"  {summary_json_path}")

print("\n" + "="*60)
print("✓ STEP 1 COMPLETE - All models analyzed!")
print("="*60)
print(f"\nNext: Run the comparison script to analyze ranking differences")

"""
STEP 2: Compare Qwen 1.5B vs Phi 3.5 Rankings
==============================================
Analyze how these two models rank the same harmful prompts
"""

import pandas as pd
import numpy as np
from scipy.stats import spearmanr, kendalltau
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os

SAVE_DIR = '/content/drive/MyDrive/LLM-refusal-directions-and-preferences/model_comparison'

print("="*60)
print("QWEN 1.5B vs PHI 3.5 COMPARISON")
print("="*60)

# ============================================================================
# LOAD RANKINGS
# ============================================================================

print("\n[1/7] Loading rankings...")

model_rankings = {}
model_info = {}

# Load both models
for model_name in ['qwen_1.5b', 'phi_3.5']:
    pickle_path = f'{SAVE_DIR}/{model_name}_results.pkl'

    with open(pickle_path, 'rb') as f:
        results = pickle.load(f)

    model_rankings[model_name] = results['rankings']
    model_info[model_name] = {
        'n_layers': results['n_layers'],
        'hidden_size': results['hidden_size'],
        'cohens_d': results['direction_info']['cohens_d'],
        'magnitude': results['direction_info']['magnitude'],
        'separation': results['direction_info']['separation'],
        'p_value': results['direction_info']['p_value']
    }

    print(f"✓ Loaded {model_name}: {len(results['rankings'])} prompts")

# ============================================================================
# MODEL INFO COMPARISON
# ============================================================================

print("\n" + "="*60)
print("MODEL CHARACTERISTICS")
print("="*60)

df_info = pd.DataFrame(model_info).T
print(df_info)

print(f"\nKey differences:")
print(f"  Qwen 1.5B: {model_info['qwen_1.5b']['n_layers']} layers, {model_info['qwen_1.5b']['hidden_size']} hidden")
print(f"  Phi 3.5:   {model_info['phi_3.5']['n_layers']} layers, {model_info['phi_3.5']['hidden_size']} hidden")
print(f"\n  Qwen Cohen's d: {model_info['qwen_1.5b']['cohens_d']:.3f}")
print(f"  Phi Cohen's d:  {model_info['phi_3.5']['cohens_d']:.3f}")

# ============================================================================
# MERGE RANKINGS
# ============================================================================

print("\n[2/7] Creating comparison table...")

df_qwen = model_rankings['qwen_1.5b'][['prompt_id', 'prompt', 'refusal_score', 'refusal_rank']].copy()
df_qwen.columns = ['prompt_id', 'prompt', 'qwen_score', 'qwen_rank']

df_phi = model_rankings['phi_3.5'][['prompt_id', 'prompt', 'refusal_score', 'refusal_rank']].copy()
df_phi.columns = ['prompt_id', 'prompt', 'phi_score', 'phi_rank']

# Merge
df_comparison = df_qwen.merge(df_phi, on=['prompt_id', 'prompt'], how='inner')

print(f"✓ Merged {len(df_comparison)} prompts")

# Add useful columns
df_comparison['rank_diff'] = df_comparison['qwen_rank'] - df_comparison['phi_rank']
df_comparison['rank_diff_abs'] = df_comparison['rank_diff'].abs()
df_comparison['mean_rank'] = (df_comparison['qwen_rank'] + df_comparison['phi_rank']) / 2
df_comparison['mean_score'] = (df_comparison['qwen_score'] + df_comparison['phi_score']) / 2

# ============================================================================
# CORRELATION ANALYSIS
# ============================================================================

print("\n[3/7] Computing correlations...")

# Spearman correlation (rank-based)
spearman_corr, spearman_p = spearmanr(df_comparison['qwen_rank'], df_comparison['phi_rank'])

# Kendall tau (rank-based, robust to outliers)
kendall_corr, kendall_p = kendalltau(df_comparison['qwen_rank'], df_comparison['phi_rank'])

# Pearson correlation (score-based)
from scipy.stats import pearsonr
pearson_corr, pearson_p = pearsonr(df_comparison['qwen_score'], df_comparison['phi_score'])

print("\n" + "="*60)
print("CORRELATION ANALYSIS")
print("="*60)
print(f"\nSpearman Rank Correlation: {spearman_corr:.4f} (p={spearman_p:.2e})")
print(f"Kendall Tau Correlation:   {kendall_corr:.4f} (p={kendall_p:.2e})")
print(f"Pearson Score Correlation: {pearson_corr:.4f} (p={pearson_p:.2e})")

if spearman_corr > 0.8:
    print("\n✓ STRONG AGREEMENT - Models rank prompts very similarly")
elif spearman_corr > 0.6:
    print("\n⚠ MODERATE AGREEMENT - Models have some differences")
else:
    print("\n✗ WEAK AGREEMENT - Models rank prompts quite differently")

# ============================================================================
# DISAGREEMENT ANALYSIS
# ============================================================================

print("\n[4/7] Analyzing disagreements...")

# Top disagreements (largest rank differences)
df_disagreements = df_comparison.sort_values('rank_diff_abs', ascending=False)

print("\n" + "="*60)
print("TOP 20 DISAGREEMENTS")
print("="*60)
print("(Prompts where models disagree most on ranking)\n")

for idx, row in df_disagreements.head(20).iterrows():
    print(f"\nPrompt: {row['prompt'][:100]}...")
    print(f"  Qwen rank: #{int(row['qwen_rank']):>4} (score: {row['qwen_score']:>7.4f})")
    print(f"  Phi rank:  #{int(row['phi_rank']):>4} (score: {row['phi_score']:>7.4f})")
    print(f"  Difference: {int(row['rank_diff'])} positions")

# ============================================================================
# CONSENSUS ANALYSIS
# ============================================================================

print("\n[5/7] Identifying consensus...")

# Strong consensus (both rank in top 10%)
n_prompts = len(df_comparison)
top_10_pct = int(n_prompts * 0.1)

df_consensus_harmful = df_comparison[
    (df_comparison['qwen_rank'] <= top_10_pct) &
    (df_comparison['phi_rank'] <= top_10_pct)
].sort_values('mean_rank')

print("\n" + "="*60)
print(f"CONSENSUS: Both models agree these are MOST harmful (top 10%)")
print("="*60)
print(f"Found {len(df_consensus_harmful)} prompts where both agree\n")

for idx, row in df_consensus_harmful.head(15).iterrows():
    print(f"\nMean rank: {row['mean_rank']:.1f}")
    print(f"  Qwen: #{int(row['qwen_rank'])}, Phi: #{int(row['phi_rank'])}")
    print(f"  Prompt: {row['prompt'][:120]}...")

# Qwen high, Phi low (Qwen thinks harmful, Phi doesn't)
qwen_unique = df_comparison[
    (df_comparison['qwen_rank'] <= top_10_pct) &
    (df_comparison['phi_rank'] > n_prompts - top_10_pct)
].sort_values('qwen_rank')

print("\n" + "="*60)
print("QWEN UNIQUE: Qwen ranks high, Phi ranks low")
print("="*60)

if len(qwen_unique) > 0:
    for idx, row in qwen_unique.head(10).iterrows():
        print(f"\nQwen rank: #{int(row['qwen_rank'])}, Phi rank: #{int(row['phi_rank'])}")
        print(f"  {row['prompt'][:120]}...")
else:
    print("No major disagreements of this type")

# Phi high, Qwen low (Phi thinks harmful, Qwen doesn't)
phi_unique = df_comparison[
    (df_comparison['phi_rank'] <= top_10_pct) &
    (df_comparison['qwen_rank'] > n_prompts - top_10_pct)
].sort_values('phi_rank')

print("\n" + "="*60)
print("PHI UNIQUE: Phi ranks high, Qwen ranks low")
print("="*60)

if len(phi_unique) > 0:
    for idx, row in phi_unique.head(10).iterrows():
        print(f"\nQwen rank: #{int(row['qwen_rank'])}, Phi rank: #{int(row['phi_rank'])}")
        print(f"  {row['prompt'][:120]}...")
else:
    print("No major disagreements of this type")

# ============================================================================
# VISUALIZATIONS
# ============================================================================

print("\n[6/7] Creating visualizations...")

fig = plt.figure(figsize=(18, 14))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# Plot 1: Rank correlation scatter
ax1 = fig.add_subplot(gs[0, :2])
scatter = ax1.scatter(df_comparison['qwen_rank'], df_comparison['phi_rank'],
                     c=df_comparison['mean_score'], cmap='RdYlGn_r',
                     alpha=0.6, s=30, edgecolors='black', linewidth=0.5)
ax1.plot([0, n_prompts], [0, n_prompts], 'r--', lw=2, label='Perfect agreement')
ax1.set_xlabel('Qwen 1.5B Rank', fontsize=12)
ax1.set_ylabel('Phi 3.5 Rank', fontsize=12)
ax1.set_title(f'Rank Correlation (Spearman: {spearman_corr:.3f})', fontsize=14, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)
plt.colorbar(scatter, ax=ax1, label='Mean Score')

# Plot 2: Rank difference distribution
ax2 = fig.add_subplot(gs[0, 2])
ax2.hist(df_comparison['rank_diff'], bins=50, color='coral', edgecolor='black', alpha=0.7)
ax2.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No difference')
ax2.set_xlabel('Rank Difference (Qwen - Phi)', fontsize=11)
ax2.set_ylabel('Count', fontsize=11)
ax2.set_title('Ranking Differences', fontsize=12, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Score distributions
ax3 = fig.add_subplot(gs[1, 0])
ax3.hist(df_comparison['qwen_score'], bins=40, alpha=0.6, label='Qwen', color='blue', edgecolor='black')
ax3.hist(df_comparison['phi_score'], bins=40, alpha=0.6, label='Phi', color='green', edgecolor='black')
ax3.set_xlabel('Refusal Score', fontsize=11)
ax3.set_ylabel('Count', fontsize=11)
ax3.set_title('Score Distributions', fontsize=12, fontweight='bold')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot 4: Score correlation
ax4 = fig.add_subplot(gs[1, 1])
ax4.scatter(df_comparison['qwen_score'], df_comparison['phi_score'],
           alpha=0.5, s=20, c='purple', edgecolors='black', linewidth=0.5)
ax4.set_xlabel('Qwen Score', fontsize=11)
ax4.set_ylabel('Phi Score', fontsize=11)
ax4.set_title(f'Score Correlation (r={pearson_corr:.3f})', fontsize=12, fontweight='bold')
ax4.grid(True, alpha=0.3)

# Plot 5: Top 50 comparison
ax5 = fig.add_subplot(gs[1, 2])
top_50_comparison = []
for i in range(min(50, len(df_comparison))):
    qwen_top = df_comparison.nsmallest(50, 'qwen_rank').iloc[i]['prompt_id']
    phi_top = df_comparison.nsmallest(50, 'phi_rank').iloc[i]['prompt_id']
    if qwen_top == phi_top:
        top_50_comparison.append('Both')
    else:
        if qwen_top in df_comparison.nsmallest(50, 'phi_rank')['prompt_id'].values:
            top_50_comparison.append('Overlap')
        else:
            top_50_comparison.append('Unique')

from collections import Counter
top_50_counts = Counter(top_50_comparison)
ax5.bar(top_50_counts.keys(), top_50_counts.values(), color=['green', 'orange', 'red'], alpha=0.7)
ax5.set_ylabel('Count', fontsize=11)
ax5.set_title('Top 50 Agreement', fontsize=12, fontweight='bold')
ax5.grid(True, alpha=0.3, axis='y')

# Plot 6: Cumulative rank difference
ax6 = fig.add_subplot(gs[2, 0])
sorted_diffs = np.sort(df_comparison['rank_diff_abs'].values)
cumulative = np.arange(1, len(sorted_diffs) + 1) / len(sorted_diffs) * 100
ax6.plot(sorted_diffs, cumulative, 'b-', linewidth=2)
ax6.axvline(x=50, color='red', linestyle='--', label='50 rank difference')
ax6.axvline(x=100, color='orange', linestyle='--', label='100 rank difference')
ax6.set_xlabel('Absolute Rank Difference', fontsize=11)
ax6.set_ylabel('Cumulative % of Prompts', fontsize=11)
ax6.set_title('Cumulative Rank Differences', fontsize=12, fontweight='bold')
ax6.legend()
ax6.grid(True, alpha=0.3)

# Plot 7: Agreement by rank tier
ax7 = fig.add_subplot(gs[2, 1:])
tier_size = n_prompts // 5
tiers = ['Top 20%', '20-40%', '40-60%', '60-80%', 'Bottom 20%']
tier_agreements = []

for i in range(5):
    tier_start = i * tier_size
    tier_end = (i + 1) * tier_size if i < 4 else n_prompts

    # Get prompts in this tier for Qwen
    qwen_tier = df_comparison.sort_values('qwen_rank').iloc[tier_start:tier_end]

    # Count how many are in same tier for Phi
    agreement = 0
    for _, row in qwen_tier.iterrows():
        phi_tier_idx = (row['phi_rank'] - 1) // tier_size
        phi_tier_idx = min(phi_tier_idx, 4)
        if phi_tier_idx == i:
            agreement += 1

    tier_agreements.append(agreement / len(qwen_tier) * 100)

ax7.bar(tiers, tier_agreements, color='steelblue', alpha=0.7, edgecolor='black')
ax7.axhline(y=20, color='red', linestyle='--', label='Random (20%)')
ax7.set_ylabel('Agreement %', fontsize=11)
ax7.set_title('Agreement by Ranking Tier', fontsize=12, fontweight='bold')
ax7.legend()
ax7.grid(True, alpha=0.3, axis='y')
plt.setp(ax7.get_xticklabels(), rotation=15, ha='right')

plt.suptitle('Qwen 1.5B vs Phi 3.5: Refusal Ranking Comparison',
             fontsize=16, fontweight='bold', y=0.995)

plt.savefig(f'{SAVE_DIR}/qwen_vs_phi_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print("✓ Visualizations created")

# ============================================================================
# SAVE RESULTS
# ============================================================================

print("\n[7/7] Saving results...")

# Full comparison
comparison_path = f'{SAVE_DIR}/qwen_vs_phi_comparison.csv'
df_comparison.to_csv(comparison_path, index=False)
print(f"✓ Full comparison: {comparison_path}")

# Disagreements
disagree_path = f'{SAVE_DIR}/qwen_vs_phi_disagreements.csv'
df_disagreements.head(100).to_csv(disagree_path, index=False)
print(f"✓ Disagreements: {disagree_path}")

# Consensus
consensus_path = f'{SAVE_DIR}/qwen_vs_phi_consensus.csv'
df_consensus_harmful.to_csv(consensus_path, index=False)
print(f"✓ Consensus: {consensus_path}")

# Summary statistics
summary = {
    'n_prompts': len(df_comparison),
    'spearman_correlation': float(spearman_corr),
    'kendall_correlation': float(kendall_corr),
    'pearson_correlation': float(pearson_corr),
    'mean_rank_difference': float(df_comparison['rank_diff_abs'].mean()),
    'median_rank_difference': float(df_comparison['rank_diff_abs'].median()),
    'max_rank_difference': float(df_comparison['rank_diff_abs'].max()),
    'consensus_top_10pct': len(df_consensus_harmful),
    'qwen_unique_harmful': len(qwen_unique),
    'phi_unique_harmful': len(phi_unique)
}

summary_path = f'{SAVE_DIR}/qwen_vs_phi_summary.json'
import json
with open(summary_path, 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Summary: {summary_path}")

print("\n" + "="*60)
print("✓ COMPARISON COMPLETE!")
print("="*60)
print(f"\nKey Findings:")
print(f"  • Spearman correlation: {spearman_corr:.4f}")
print(f"  • Mean rank difference: {df_comparison['rank_diff_abs'].mean():.1f} positions")
print(f"  • Prompts with consensus (both top 10%): {len(df_consensus_harmful)}")
print(f"  • Qwen unique concerns: {len(qwen_unique)}")
print(f"  • Phi unique concerns: {len(phi_unique)}")

"""
STEP 3: Semantic Analysis of Disagreements
===========================================
Analyze what types of content Qwen vs Phi disagree on most
"""

import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import re
import matplotlib.pyplot as plt
import seaborn as sns

SAVE_DIR = '/content/drive/MyDrive/LLM-refusal-directions-and-preferences/model_comparison'

print("="*60)
print("SEMANTIC ANALYSIS OF DISAGREEMENTS")
print("="*60)

# ============================================================================
# LOAD DATA
# ============================================================================

print("\n[1/6] Loading comparison data...")

df_comparison = pd.read_csv(f'{SAVE_DIR}/qwen_vs_phi_comparison.csv')
print(f"✓ Loaded {len(df_comparison)} prompts")

# ============================================================================
# DEFINE HARM CATEGORIES
# ============================================================================

print("\n[2/6] Categorizing prompts by harm type...")

# Comprehensive harm categories
harm_categories = {
    'violence_physical': [
        'kill', 'murder', 'assassinate', 'shoot', 'stab', 'torture', 'hurt',
        'attack', 'beating', 'poison', 'strangle', 'drown', 'suffocate', 'punch',
        'fight', 'assault', 'harm', 'injure', 'wound', 'maim'
    ],
    'weapons_explosives': [
        'bomb', 'explosive', 'weapon', 'gun', 'knife', 'grenade', 'ammunition',
        'firearm', 'detonate', 'blast', 'missile', 'dynamite', 'c4', 'ied',
        'rifle', 'pistol', 'molotov'
    ],
    'illegal_drugs': [
        'drug', 'cocaine', 'heroin', 'meth', 'methamphetamine', 'narcotic',
        'trafficking', 'dealer', 'smuggle', 'cannabis', 'fentanyl', 'mdma',
        'ecstasy', 'lsd', 'marijuana', 'opioid', 'cartel'
    ],
    'child_harm': [
        'child', 'children', 'kid', 'minor', 'underage', 'baby', 'infant',
        'teenager', 'youth', 'juvenile', 'toddler', 'adolescent', 'schoolchild'
    ],
    'sexual_content': [
        'sex', 'sexual', 'rape', 'assault', 'abuse', 'molest', 'pornography',
        'inappropriate', 'explicit', 'prostitution', 'trafficking', 'pedophile',
        'incest', 'grooming'
    ],
    'theft_fraud': [
        'steal', 'theft', 'rob', 'fraud', 'scam', 'embezzle', 'counterfeit',
        'forge', 'hack', 'phishing', 'burglary', 'shoplift', 'pickpocket',
        'identity theft', 'credit card'
    ],
    'suicide_selfharm': [
        'suicide', 'kill myself', 'self-harm', 'self harm', 'cutting', 'overdose',
        'end my life', 'harm myself', 'take my own life', 'jump off', 'hang myself'
    ],
    'hate_discrimination': [
        'racist', 'sexist', 'hate', 'discrimination', 'slur', 'bigot', 'supremacy',
        'genocide', 'ethnic cleansing', 'nazi', 'holocaust', 'apartheid', 'xenophobia'
    ],
    'manipulation_deception': [
        'manipulate', 'gaslight', 'deceive', 'trick', 'coerce', 'blackmail',
        'extort', 'threaten', 'intimidate', 'bribe', 'corrupt', 'exploit',
        'brainwash', 'propaganda'
    ],
    'illegal_general': [
        'illegal', 'crime', 'criminal', 'unlawful', 'illicit', 'prohibited',
        'banned', 'contraband', 'smuggling', 'laundering', 'trafficking'
    ],
    'terrorism': [
        'terrorist', 'terrorism', 'attack', 'bombing', 'hostage', 'hijack',
        'extremist', 'radicalize', 'isis', 'al qaeda', 'militia'
    ],
    'instructions_howto': [
        'how to', 'step by step', 'instructions', 'tutorial', 'guide to',
        'make a', 'create a', 'build a', 'produce', 'manufacture'
    ],
    'evasion_avoidance': [
        'get away with', 'avoid detection', 'not get caught', 'evade', 'escape',
        'hide', 'conceal', 'without being caught', 'undetected'
    ]
}

def categorize_prompt(prompt):
    """Assign categories to a prompt"""
    prompt_lower = prompt.lower()
    categories = []

    for category, keywords in harm_categories.items():
        for keyword in keywords:
            if keyword in prompt_lower:
                categories.append(category)
                break  # Only count category once

    return categories if categories else ['other']

# Apply categorization
df_comparison['categories'] = df_comparison['prompt'].apply(categorize_prompt)
df_comparison['primary_category'] = df_comparison['categories'].apply(lambda x: x[0])
df_comparison['n_categories'] = df_comparison['categories'].apply(len)

print("✓ Categorization complete")

# ============================================================================
# ANALYZE TOP DISAGREEMENTS
# ============================================================================

print("\n[3/6] Analyzing semantic patterns in disagreements...")

# Split into groups by disagreement type
n_prompts = len(df_comparison)
threshold = 150  # Rank difference threshold

# Qwen thinks MORE harmful than Phi (negative rank_diff)
qwen_more_harmful = df_comparison[df_comparison['rank_diff'] < -threshold].sort_values('rank_diff')

# Phi thinks MORE harmful than Qwen (positive rank_diff)
phi_more_harmful = df_comparison[df_comparison['rank_diff'] > threshold].sort_values('rank_diff', ascending=False)

print(f"\n✓ Prompts where Qwen ranks much higher: {len(qwen_more_harmful)}")
print(f"✓ Prompts where Phi ranks much higher: {len(phi_more_harmful)}")

# ============================================================================
# CATEGORY ANALYSIS BY DISAGREEMENT
# ============================================================================

print("\n[4/6] Comparing category emphasis...")

# Category counts for each disagreement type
qwen_categories = []
for cats in qwen_more_harmful['categories']:
    qwen_categories.extend(cats)

phi_categories = []
for cats in phi_more_harmful['categories']:
    phi_categories.extend(cats)

all_categories = []
for cats in df_comparison['categories']:
    all_categories.extend(cats)

qwen_cat_counts = Counter(qwen_categories)
phi_cat_counts = Counter(phi_categories)
all_cat_counts = Counter(all_categories)

# Create comparison dataframe
category_comparison = []
for cat in harm_categories.keys():
    qwen_count = qwen_cat_counts.get(cat, 0)
    phi_count = phi_cat_counts.get(cat, 0)
    all_count = all_cat_counts.get(cat, 0)

    # Normalized by total in each group
    qwen_pct = (qwen_count / len(qwen_more_harmful) * 100) if len(qwen_more_harmful) > 0 else 0
    phi_pct = (phi_count / len(phi_more_harmful) * 100) if len(phi_more_harmful) > 0 else 0
    baseline_pct = (all_count / len(df_comparison) * 100)

    category_comparison.append({
        'category': cat,
        'qwen_count': qwen_count,
        'phi_count': phi_count,
        'qwen_pct': qwen_pct,
        'phi_pct': phi_pct,
        'baseline_pct': baseline_pct,
        'qwen_emphasis': qwen_pct - baseline_pct,
        'phi_emphasis': phi_pct - baseline_pct
    })

df_cat_comp = pd.DataFrame(category_comparison).sort_values('qwen_emphasis', ascending=False)

print("\n" + "="*60)
print("CATEGORY EMPHASIS COMPARISON")
print("="*60)
print("\nCategories QWEN emphasizes more (thinks more harmful):")
print(df_cat_comp.nlargest(8, 'qwen_emphasis')[['category', 'qwen_pct', 'phi_pct', 'qwen_emphasis']])

print("\n\nCategories PHI emphasizes more (thinks more harmful):")
print(df_cat_comp.nlargest(8, 'phi_emphasis')[['category', 'phi_pct', 'qwen_pct', 'phi_emphasis']])

# ============================================================================
# SHOW ACTUAL PROMPTS
# ============================================================================

print("\n[5/6] Examining specific prompts...")

print("\n" + "="*60)
print("QWEN'S UNIQUE CONCERNS (Qwen ranks HIGH, Phi ranks LOW)")
print("="*60)
print("These are prompts Qwen thinks are dangerous but Phi doesn't\n")

for idx, row in qwen_more_harmful.head(20).iterrows():
    print(f"\n{'-'*70}")
    print(f"Qwen rank: #{int(row['qwen_rank'])} | Phi rank: #{int(row['phi_rank'])} | Diff: {int(row['rank_diff'])}")
    print(f"Categories: {', '.join(row['categories'])}")
    print(f"Prompt: {row['prompt']}")
    print(f"Scores - Qwen: {row['qwen_score']:.3f}, Phi: {row['phi_score']:.3f}")

print("\n" + "="*60)
print("PHI'S UNIQUE CONCERNS (Phi ranks HIGH, Qwen ranks LOW)")
print("="*60)
print("These are prompts Phi thinks are dangerous but Qwen doesn't\n")

for idx, row in phi_more_harmful.head(20).iterrows():
    print(f"\n{'-'*70}")
    print(f"Qwen rank: #{int(row['qwen_rank'])} | Phi rank: #{int(row['phi_rank'])} | Diff: {int(row['rank_diff'])}")
    print(f"Categories: {', '.join(row['categories'])}")
    print(f"Prompt: {row['prompt']}")
    print(f"Scores - Qwen: {row['qwen_score']:.3f}, Phi: {row['phi_score']:.3f}")

# ============================================================================
# LINGUISTIC PATTERN ANALYSIS
# ============================================================================

print("\n" + "="*60)
print("LINGUISTIC PATTERNS")
print("="*60)

def extract_action_verbs(prompts):
    """Extract first verb/action word from prompts"""
    verbs = []
    for prompt in prompts:
        # Common harmful action patterns
        words = prompt.lower().split()
        for word in words[:10]:  # Check first 10 words
            if word in ['create', 'make', 'build', 'design', 'develop', 'write',
                       'generate', 'produce', 'manufacture', 'construct',
                       'kill', 'hurt', 'harm', 'attack', 'destroy', 'steal',
                       'hack', 'break', 'forge', 'counterfeit', 'scam',
                       'manipulate', 'trick', 'deceive', 'exploit']:
                verbs.append(word)
                break
    return Counter(verbs)

qwen_verbs = extract_action_verbs(qwen_more_harmful['prompt'])
phi_verbs = extract_action_verbs(phi_more_harmful['prompt'])

print("\nTop action verbs in QWEN's unique concerns:")
for verb, count in qwen_verbs.most_common(10):
    print(f"  {verb}: {count}")

print("\nTop action verbs in PHI's unique concerns:")
for verb, count in phi_verbs.most_common(10):
    print(f"  {verb}: {count}")

# Check for "how to" vs direct requests
qwen_howto = sum(1 for p in qwen_more_harmful['prompt'] if 'how to' in p.lower() or 'how can' in p.lower())
phi_howto = sum(1 for p in phi_more_harmful['prompt'] if 'how to' in p.lower() or 'how can' in p.lower())

print(f"\n\"How to\" questions:")
print(f"  Qwen concerns: {qwen_howto}/{len(qwen_more_harmful)} ({qwen_howto/len(qwen_more_harmful)*100:.1f}%)")
print(f"  Phi concerns: {phi_howto}/{len(phi_more_harmful)} ({phi_howto/len(phi_more_harmful)*100:.1f}%)")

# ============================================================================
# VISUALIZATIONS
# ============================================================================

print("\n[6/6] Creating visualizations...")

fig, axes = plt.subplots(2, 2, figsize=(18, 12))

# Plot 1: Category emphasis comparison
ax1 = axes[0, 0]
categories_to_plot = df_cat_comp.head(10)['category'].tolist()
x = np.arange(len(categories_to_plot))
width = 0.35

qwen_bars = [df_cat_comp[df_cat_comp['category']==c]['qwen_pct'].values[0] for c in categories_to_plot]
phi_bars = [df_cat_comp[df_cat_comp['category']==c]['phi_pct'].values[0] for c in categories_to_plot]

ax1.barh(x - width/2, qwen_bars, width, label='Qwen emphasis', color='blue', alpha=0.7)
ax1.barh(x + width/2, phi_bars, width, label='Phi emphasis', color='green', alpha=0.7)
ax1.set_yticks(x)
ax1.set_yticklabels([c.replace('_', ' ').title() for c in categories_to_plot], fontsize=9)
ax1.set_xlabel('% of Disagreement Prompts', fontsize=11)
ax1.set_title('Category Emphasis in Disagreements', fontsize=12, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3, axis='x')

# Plot 2: Emphasis difference
ax2 = axes[0, 1]
emphasis_diff = df_cat_comp.sort_values('qwen_emphasis', ascending=True).tail(15)
colors = ['blue' if x > 0 else 'green' for x in emphasis_diff['qwen_emphasis']]
ax2.barh(range(len(emphasis_diff)), emphasis_diff['qwen_emphasis'], color=colors, alpha=0.7)
ax2.set_yticks(range(len(emphasis_diff)))
ax2.set_yticklabels([c.replace('_', ' ').title() for c in emphasis_diff['category']], fontsize=9)
ax2.axvline(x=0, color='red', linestyle='--', linewidth=2)
ax2.set_xlabel('Emphasis Difference (% above baseline)', fontsize=11)
ax2.set_title('Qwen (Blue) vs Phi (Green) Category Bias', fontsize=12, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='x')

# Plot 3: Disagreement by category
ax3 = axes[1, 0]
category_disagreements = df_comparison.groupby('primary_category').agg({
    'rank_diff_abs': 'mean'
}).sort_values('rank_diff_abs', ascending=False).head(12)

ax3.barh(range(len(category_disagreements)),
         category_disagreements['rank_diff_abs'],
         color='coral', alpha=0.7, edgecolor='black')
ax3.set_yticks(range(len(category_disagreements)))
ax3.set_yticklabels([c.replace('_', ' ').title() for c in category_disagreements.index], fontsize=9)
ax3.set_xlabel('Mean Absolute Rank Difference', fontsize=11)
ax3.set_title('Which Categories Have Most Disagreement?', fontsize=12, fontweight='bold')
ax3.grid(True, alpha=0.3, axis='x')

# Plot 4: Category distribution in top disagreements
ax4 = axes[1, 1]
top_disagreements = df_comparison.nlargest(100, 'rank_diff_abs')
top_cats = []
for cats in top_disagreements['categories']:
    top_cats.extend(cats)
top_cat_counts = Counter(top_cats).most_common(12)

ax4.bar(range(len(top_cat_counts)),
        [c[1] for c in top_cat_counts],
        color='purple', alpha=0.7, edgecolor='black')
ax4.set_xticks(range(len(top_cat_counts)))
ax4.set_xticklabels([c[0].replace('_', '\n') for c in top_cat_counts],
                     fontsize=8, rotation=45, ha='right')
ax4.set_ylabel('Count in Top 100 Disagreements', fontsize=11)
ax4.set_title('Categories Most Present in Largest Disagreements', fontsize=12, fontweight='bold')
ax4.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(f'{SAVE_DIR}/semantic_analysis_disagreements.png', dpi=300, bbox_inches='tight')
plt.show()

# ============================================================================
# SAVE DETAILED ANALYSIS
# ============================================================================

print("\nSaving detailed analysis...")

# Save Qwen's unique concerns
qwen_unique_path = f'{SAVE_DIR}/qwen_unique_concerns.csv'
qwen_more_harmful.to_csv(qwen_unique_path, index=False)
print(f"✓ Qwen unique concerns: {qwen_unique_path}")

# Save Phi's unique concerns
phi_unique_path = f'{SAVE_DIR}/phi_unique_concerns.csv'
phi_more_harmful.to_csv(phi_unique_path, index=False)
print(f"✓ Phi unique concerns: {phi_unique_path}")

# Save category comparison
cat_comp_path = f'{SAVE_DIR}/category_comparison_detailed.csv'
df_cat_comp.to_csv(cat_comp_path, index=False)
print(f"✓ Category comparison: {cat_comp_path}")

print("\n" + "="*60)
print("✓ SEMANTIC ANALYSIS COMPLETE!")
print("="*60)

"""
EXPERIMENT 1B: Expand Model Comparison
======================================
Add Phi-2 and Gemma-2B to the analysis
"""

import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from scipy.stats import ttest_ind
from tqdm import tqdm
import pickle
import json
import os

SAVE_DIR = '/content/drive/MyDrive/LLM-refusal-directions-and-preferences/model_comparison'

# ============================================================================
# NEW MODELS TO ADD
# ============================================================================

NEW_MODELS = {
    'phi_2.7b': 'microsoft/phi-2',
    'gemma_2b': 'google/gemma-2-2b-it',
}

print("="*60)
print("ADDING NEW MODELS TO COMPARISON")
print("="*60)
for name, path in NEW_MODELS.items():
    print(f"  {name}: {path}")

# ============================================================================
# LOAD DATASETS
# ============================================================================

print("\n" + "="*60)
print("LOADING DATASETS")
print("="*60)

# Load datasets (same as before)
harmful_ds = load_dataset('mlabonne/harmful_behaviors')
all_harmful_prompts = harmful_ds['train']['text']

N_HARMFUL_TRAIN = 256
harmful_train = all_harmful_prompts[:N_HARMFUL_TRAIN]
harmful_test = all_harmful_prompts

harmless_ds = load_dataset('mlabonne/harmless_alpaca')
harmless_train = harmless_ds['train']['text'][:256]

print(f"✓ Harmful train: {len(harmful_train)}")
print(f"✓ Harmful test: {len(harmful_test)}")
print(f"✓ Harmless train: {len(harmless_train)}")

# ============================================================================
# HELPER FUNCTIONS (same as before)
# ============================================================================

def get_activation(prompt, model, tokenizer, layer_idx):
    """Extract activation at last token position for specific layer"""
    messages = [{"role": "user", "content": prompt}]
    formatted = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(
        formatted,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(model.device)

    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)

    last_pos = inputs['input_ids'].shape[1] - 1
    activation = outputs.hidden_states[layer_idx + 1][0, last_pos, :].cpu().numpy()

    return activation

def compute_refusal_direction(model, tokenizer, harmful_prompts, harmless_prompts):
    """Compute refusal direction for FINAL layer"""
    n_layers = len(model.model.layers)
    final_layer_idx = n_layers - 1

    print(f"\nComputing refusal direction at final layer ({final_layer_idx})...")

    harmful_acts = []
    harmless_acts = []

    for prompt in tqdm(harmful_prompts, desc="Processing harmful"):
        act = get_activation(prompt, model, tokenizer, final_layer_idx)
        harmful_acts.append(act)

    for prompt in tqdm(harmless_prompts, desc="Processing harmless"):
        act = get_activation(prompt, model, tokenizer, final_layer_idx)
        harmless_acts.append(act)

    harmful_mean = np.array(harmful_acts).mean(axis=0)
    harmless_mean = np.array(harmless_acts).mean(axis=0)
    direction = harmful_mean - harmless_mean

    direction_norm = direction / np.linalg.norm(direction)
    magnitude = np.linalg.norm(direction)

    harmful_projections = [np.dot(act, direction_norm) for act in harmful_acts]
    harmless_projections = [np.dot(act, direction_norm) for act in harmless_acts]

    t_stat, p_val = ttest_ind(harmful_projections, harmless_projections)

    mean_diff = np.mean(harmful_projections) - np.mean(harmless_projections)
    pooled_std = np.sqrt(
        (np.std(harmful_projections)**2 + np.std(harmless_projections)**2) / 2
    )
    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0

    print(f"\n✓ Direction computed!")
    print(f"  Layer: {final_layer_idx}/{n_layers-1}")
    print(f"  Magnitude: {magnitude:.3f}")
    print(f"  Cohen's d: {cohens_d:.3f}")
    print(f"  p-value: {p_val:.2e}")

    return {
        'layer': final_layer_idx,
        'direction': direction_norm,
        'magnitude': magnitude,
        'cohens_d': cohens_d,
        'p_value': p_val,
        'harmful_mean_proj': np.mean(harmful_projections),
        'harmless_mean_proj': np.mean(harmless_projections),
        'separation': mean_diff
    }

def rank_harmful_prompts(model, tokenizer, prompts, direction_info):
    """Score and rank all harmful prompts"""
    direction = direction_info['direction']
    layer_idx = direction_info['layer']

    print(f"\nRanking {len(prompts)} harmful prompts...")

    scores = []
    for prompt in tqdm(prompts, desc="Scoring prompts"):
        try:
            act = get_activation(prompt, model, tokenizer, layer_idx)
            score = np.dot(act, direction)
            scores.append(score)
        except Exception as e:
            print(f"\nError: {e}")
            scores.append(np.nan)

    df_results = pd.DataFrame({
        'prompt_id': range(len(prompts)),
        'prompt': prompts,
        'refusal_score': scores
    })

    df_results = df_results.dropna(subset=['refusal_score'])
    df_results['refusal_rank'] = df_results['refusal_score'].rank(ascending=False, method='min').astype(int)
    df_results = df_results.sort_values('refusal_rank').reset_index(drop=True)

    print(f"✓ Ranked {len(df_results)} prompts")
    print(f"  Score range: [{df_results['refusal_score'].min():.4f}, {df_results['refusal_score'].max():.4f}]")

    return df_results

# ============================================================================
# ANALYZE NEW MODELS
# ============================================================================

print("\n" + "="*60)
print("ANALYZING NEW MODELS")
print("="*60)

all_results = {}

for model_name, model_path in NEW_MODELS.items():

    print("\n" + "="*70)
    print(f"MODEL: {model_name}")
    print("="*70)

    try:
        # Load model
        print(f"\n[1/4] Loading model...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        # Handle models without chat template
        if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:
            print("⚠ No chat template found, using default formatting")
            # Set a simple default template
            tokenizer.chat_template = "{% for message in messages %}{{ message['content'] }}{% endfor %}"

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True  # Some models need this
        )

        n_layers = len(model.model.layers)
        hidden_size = model.config.hidden_size

        print(f"✓ Model loaded")
        print(f"  Architecture: {model.config.model_type}")
        print(f"  Layers: {n_layers}")
        print(f"  Hidden size: {hidden_size}")

        # Compute direction
        print(f"\n[2/4] Computing refusal direction...")
        direction_info = compute_refusal_direction(
            model, tokenizer, harmful_train, harmless_train
        )

        # Rank prompts
        print(f"\n[3/4] Ranking harmful prompts...")
        df_rankings = rank_harmful_prompts(
            model, tokenizer, harmful_test, direction_info
        )

        # Save results
        print(f"\n[4/4] Saving results...")

        results = {
            'model_name': model_name,
            'model_path': model_path,
            'n_layers': n_layers,
            'hidden_size': hidden_size,
            'direction_info': direction_info,
            'rankings': df_rankings
        }

        pickle_path = f'{SAVE_DIR}/{model_name}_results.pkl'
        with open(pickle_path, 'wb') as f:
            pickle.dump(results, f)

        csv_path = f'{SAVE_DIR}/{model_name}_rankings.csv'
        df_rankings.to_csv(csv_path, index=False)

        print(f"✓ Saved to:")
        print(f"  {pickle_path}")
        print(f"  {csv_path}")

        # Show top 10
        print(f"\nTop 10 highest refusal scores:")
        for idx, row in df_rankings.head(10).iterrows():
            print(f"  Rank {row['refusal_rank']}: {row['refusal_score']:.4f} - {row['prompt'][:80]}...")

        all_results[model_name] = results

        # Cleanup
        del model
        torch.cuda.empty_cache()

        print(f"\n✓ {model_name} complete!")

    except Exception as e:
        print(f"\n✗ ERROR with {model_name}: {e}")
        import traceback
        traceback.print_exc()
        continue

# ============================================================================
# UPDATE SUMMARY
# ============================================================================

print("\n" + "="*60)
print("UPDATING SUMMARY")
print("="*60)

# Load existing summary
summary_path = f'{SAVE_DIR}/model_summary.csv'
if os.path.exists(summary_path):
    df_summary = pd.read_csv(summary_path)
else:
    df_summary = pd.DataFrame()

# Add new results
for model_name, results in all_results.items():
    new_row = {
        'model': model_name,
        'n_layers': results['n_layers'],
        'hidden_size': results['hidden_size'],
        'cohens_d': results['direction_info']['cohens_d'],
        'p_value': results['direction_info']['p_value'],
        'magnitude': results['direction_info']['magnitude'],
        'separation': results['direction_info']['separation'],
        'n_prompts_ranked': len(results['rankings'])
    }
    df_summary = pd.concat([df_summary, pd.DataFrame([new_row])], ignore_index=True)

# Save updated summary
df_summary.to_csv(summary_path, index=False)

print("\n✓ Updated Model Summary:")
print(df_summary.to_string(index=False))

print(f"\n✓ Summary saved to: {summary_path}")

print("\n" + "="*60)
print("✓ NEW MODELS ANALYZED!")
print("="*60)
print(f"\nTotal models now: {len(df_summary)}")
print(f"  - Qwen 1.5B")
print(f"  - Phi 3.5")
print(f"  - Phi 2.7B")
print(f"  - Gemma 2B")

# Commented out IPython magic to ensure Python compatibility.
"""
STEP 1: Generate Rankings for All Four Models
==============================================
Models: Qwen 1.5B, Phi 3.5, Phi 2.7B, Gemma 2B
"""

# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Navigate
# %cd /content/drive/MyDrive/LLM-refusal-directions-and-preferences

import sys
sys.path.append('/content/drive/MyDrive/LLM-refusal-directions-and-preferences')

# Imports
import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from scipy.stats import ttest_ind
from tqdm import tqdm
import pickle
import json
import os

print("✓ All imports successful")
print(f"✓ CUDA available: {torch.cuda.is_available()}")

# ============================================================================
# CONFIGURATION
# ============================================================================

ALL_MODELS = {
    'qwen_1.5b': 'Qwen/Qwen2.5-1.5B-Instruct',
    'phi_3.5': 'microsoft/Phi-3.5-mini-instruct',
    'phi_2.7b': 'microsoft/phi-2',
    'gemma_2b': 'google/gemma-2-2b-it',
}

SAVE_DIR = '/content/drive/MyDrive/LLM-refusal-directions-and-preferences/model_comparison'
os.makedirs(SAVE_DIR, exist_ok=True)

N_HARMFUL_TRAIN = 256
N_HARMLESS_TRAIN = 256

print("\n" + "="*60)
print("CONFIGURATION")
print("="*60)
print(f"Models to analyze: {len(ALL_MODELS)}")
for name, path in ALL_MODELS.items():
    print(f"  • {name}: {path}")
print(f"\nSave directory: {SAVE_DIR}")

# ============================================================================
# LOAD DATASETS (ONCE)
# ============================================================================

print("\n" + "="*60)
print("LOADING DATASETS")
print("="*60)

# Harmful behaviors
harmful_ds = load_dataset('mlabonne/harmful_behaviors')
all_harmful_prompts = harmful_ds['train']['text']
harmful_train = all_harmful_prompts[:N_HARMFUL_TRAIN]
harmful_test = all_harmful_prompts  # Rank all

# Harmless behaviors
harmless_ds = load_dataset('mlabonne/harmless_alpaca')
harmless_train = harmless_ds['train']['text'][:N_HARMLESS_TRAIN]

print(f"✓ Harmful train (for direction): {len(harmful_train)}")
print(f"✓ Harmful test (to rank): {len(harmful_test)}")
print(f"✓ Harmless train (for direction): {len(harmless_train)}")

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_activation(prompt, model, tokenizer, layer_idx):
    """Extract activation at last token position for specific layer"""
    messages = [{"role": "user", "content": prompt}]

    try:
        formatted = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    except:
        # Fallback for models without chat template
        formatted = prompt

    inputs = tokenizer(
        formatted,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(model.device)

    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)

    last_pos = inputs['input_ids'].shape[1] - 1
    activation = outputs.hidden_states[layer_idx + 1][0, last_pos, :].cpu().numpy()

    return activation

def compute_refusal_direction(model, tokenizer, harmful_prompts, harmless_prompts):
    """Compute refusal direction for FINAL layer"""
    n_layers = len(model.model.layers)
    final_layer_idx = n_layers - 1

    print(f"\nComputing refusal direction at final layer ({final_layer_idx})...")

    harmful_acts = []
    harmless_acts = []

    for prompt in tqdm(harmful_prompts, desc="Harmful"):
        act = get_activation(prompt, model, tokenizer, final_layer_idx)
        harmful_acts.append(act)

    for prompt in tqdm(harmless_prompts, desc="Harmless"):
        act = get_activation(prompt, model, tokenizer, final_layer_idx)
        harmless_acts.append(act)

    harmful_mean = np.array(harmful_acts).mean(axis=0)
    harmless_mean = np.array(harmless_acts).mean(axis=0)
    direction = harmful_mean - harmless_mean

    direction_norm = direction / np.linalg.norm(direction)
    magnitude = np.linalg.norm(direction)

    harmful_projections = [np.dot(act, direction_norm) for act in harmful_acts]
    harmless_projections = [np.dot(act, direction_norm) for act in harmless_acts]

    t_stat, p_val = ttest_ind(harmful_projections, harmless_projections)

    mean_diff = np.mean(harmful_projections) - np.mean(harmless_projections)
    pooled_std = np.sqrt(
        (np.std(harmful_projections)**2 + np.std(harmless_projections)**2) / 2
    )
    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0

    print(f"✓ Direction computed!")
    print(f"  Magnitude: {magnitude:.3f}")
    print(f"  Cohen's d: {cohens_d:.3f}")
    print(f"  p-value: {p_val:.2e}")
    print(f"  Valid: {'✓' if (p_val < 0.01 and abs(cohens_d) > 0.5) else '✗'}")

    return {
        'layer': final_layer_idx,
        'direction': direction_norm,
        'magnitude': magnitude,
        'cohens_d': cohens_d,
        'p_value': p_val,
        'harmful_mean_proj': np.mean(harmful_projections),
        'harmless_mean_proj': np.mean(harmless_projections),
        'separation': mean_diff
    }

def rank_harmful_prompts(model, tokenizer, prompts, direction_info):
    """Score and rank all harmful prompts"""
    direction = direction_info['direction']
    layer_idx = direction_info['layer']

    print(f"\nRanking {len(prompts)} prompts...")

    scores = []
    for prompt in tqdm(prompts, desc="Scoring"):
        try:
            act = get_activation(prompt, model, tokenizer, layer_idx)
            score = np.dot(act, direction)
            scores.append(score)
        except Exception as e:
            print(f"\nError: {e}")
            scores.append(np.nan)

    df_results = pd.DataFrame({
        'prompt_id': range(len(prompts)),
        'prompt': prompts,
        'refusal_score': scores
    })

    df_results = df_results.dropna(subset=['refusal_score'])
    df_results['refusal_rank'] = df_results['refusal_score'].rank(ascending=False, method='min').astype(int)
    df_results = df_results.sort_values('refusal_rank').reset_index(drop=True)

    print(f"✓ Ranked {len(df_results)} prompts")
    print(f"  Score range: [{df_results['refusal_score'].min():.4f}, {df_results['refusal_score'].max():.4f}]")

    return df_results

# ============================================================================
# MAIN LOOP: ANALYZE ALL MODELS
# ============================================================================

print("\n" + "="*60)
print("ANALYZING ALL MODELS")
print("="*60)

all_results = {}
summary_data = []

for i, (model_name, model_path) in enumerate(ALL_MODELS.items(), 1):

    print("\n" + "="*70)
    print(f"MODEL {i}/{len(ALL_MODELS)}: {model_name}")
    print("="*70)

    # Check if already processed
    pickle_path = f'{SAVE_DIR}/{model_name}_results.pkl'
    if os.path.exists(pickle_path):
        print(f"⚠ {model_name} already processed. Loading existing results...")
        with open(pickle_path, 'rb') as f:
            results = pickle.load(f)
        all_results[model_name] = results

        summary_data.append({
            'model': model_name,
            'n_layers': results['n_layers'],
            'hidden_size': results['hidden_size'],
            'cohens_d': results['direction_info']['cohens_d'],
            'p_value': results['direction_info']['p_value'],
            'magnitude': results['direction_info']['magnitude'],
            'separation': results['direction_info']['separation'],
            'n_prompts_ranked': len(results['rankings'])
        })

        print(f"✓ Loaded from cache")
        continue

    try:
        # [1/4] Load model
        print(f"\n[1/4] Loading model...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        # Handle models without chat template
        if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:
            print("  ⚠ No chat template, using simple formatting")

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        n_layers = len(model.model.layers)
        hidden_size = model.config.hidden_size

        print(f"✓ Model loaded")
        print(f"  Architecture: {model.config.model_type}")
        print(f"  Layers: {n_layers}")
        print(f"  Hidden size: {hidden_size}")

        # [2/4] Compute direction
        print(f"\n[2/4] Computing refusal direction...")
        direction_info = compute_refusal_direction(
            model, tokenizer, harmful_train, harmless_train
        )

        # [3/4] Rank prompts
        print(f"\n[3/4] Ranking all harmful prompts...")
        df_rankings = rank_harmful_prompts(
            model, tokenizer, harmful_test, direction_info
        )

        # [4/4] Save
        print(f"\n[4/4] Saving results...")

        results = {
            'model_name': model_name,
            'model_path': model_path,
            'n_layers': n_layers,
            'hidden_size': hidden_size,
            'direction_info': direction_info,
            'rankings': df_rankings
        }

        with open(pickle_path, 'wb') as f:
            pickle.dump(results, f)

        csv_path = f'{SAVE_DIR}/{model_name}_rankings.csv'
        df_rankings.to_csv(csv_path, index=False)

        print(f"✓ Saved:")
        print(f"  Pickle: {pickle_path}")
        print(f"  CSV: {csv_path}")

        # Top 5
        print(f"\nTop 5 highest refusal:")
        for idx, row in df_rankings.head(5).iterrows():
            print(f"  #{row['refusal_rank']}: {row['prompt'][:70]}...")

        all_results[model_name] = results

        summary_data.append({
            'model': model_name,
            'n_layers': n_layers,
            'hidden_size': hidden_size,
            'cohens_d': direction_info['cohens_d'],
            'p_value': direction_info['p_value'],
            'magnitude': direction_info['magnitude'],
            'separation': direction_info['separation'],
            'n_prompts_ranked': len(df_rankings)
        })

        # Cleanup
        del model
        torch.cuda.empty_cache()

        print(f"\n✓ {model_name} complete!")

    except Exception as e:
        print(f"\n✗ ERROR with {model_name}: {e}")
        import traceback
        traceback.print_exc()
        continue

# ============================================================================
# SAVE SUMMARY
# ============================================================================

print("\n" + "="*60)
print("FINAL SUMMARY")
print("="*60)

df_summary = pd.DataFrame(summary_data)
print(f"\n✓ Successfully processed: {len(df_summary)}/{len(ALL_MODELS)} models\n")
print(df_summary.to_string(index=False))

summary_path = f'{SAVE_DIR}/model_summary.csv'
df_summary.to_csv(summary_path, index=False)

summary_json = {
    'models_analyzed': list(all_results.keys()),
    'total_models': len(ALL_MODELS),
    'n_harmful_test': len(harmful_test),
    'n_harmful_train': len(harmful_train),
    'n_harmless_train': len(harmless_train)
}

json_path = f'{SAVE_DIR}/analysis_summary.json'
with open(json_path, 'w') as f:
    json.dump(summary_json, f, indent=2)

print(f"\n✓ Summary saved:")
print(f"  {summary_path}")
print(f"  {json_path}")

print("\n" + "="*60)
print("✓ STEP 1 COMPLETE - All Rankings Generated!")
print("="*60)
print("\nNext: Run the comparison script")

"""
STEP 3: Individual Model Visualizations
========================================
Create detailed visualizations for each model (excluding Gemma)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
from collections import Counter

SAVE_DIR = '/content/drive/MyDrive/LLM-refusal-directions-and-preferences/model_comparison'

# Models to visualize
MODELS_TO_VISUALIZE = ['qwen_1.5b', 'phi_3.5', 'phi_2.7b']

print("="*60)
print("INDIVIDUAL MODEL VISUALIZATIONS")
print("="*60)

# ============================================================================
# DEFINE HARM CATEGORIES (for semantic analysis)
# ============================================================================

harm_categories = {
    'violence_physical': [
        'kill', 'murder', 'assassinate', 'shoot', 'stab', 'torture', 'hurt',
        'attack', 'beating', 'poison', 'strangle', 'fight', 'assault', 'harm'
    ],
    'weapons_explosives': [
        'bomb', 'explosive', 'weapon', 'gun', 'knife', 'grenade', 'ammunition',
        'firearm', 'detonate', 'blast', 'missile', 'dynamite'
    ],
    'illegal_drugs': [
        'drug', 'cocaine', 'heroin', 'meth', 'methamphetamine', 'narcotic',
        'trafficking', 'dealer', 'smuggle', 'cannabis', 'fentanyl'
    ],
    'child_harm': [
        'child', 'children', 'kid', 'minor', 'underage', 'baby', 'infant',
        'teenager', 'youth', 'juvenile'
    ],
    'sexual_content': [
        'sex', 'sexual', 'rape', 'assault', 'abuse', 'molest', 'pornography',
        'explicit', 'prostitution'
    ],
    'theft_fraud': [
        'steal', 'theft', 'rob', 'fraud', 'scam', 'embezzle', 'counterfeit',
        'forge', 'hack', 'phishing'
    ],
    'suicide_selfharm': [
        'suicide', 'kill myself', 'self-harm', 'self harm', 'cutting',
        'overdose', 'end my life'
    ],
    'hate_discrimination': [
        'racist', 'sexist', 'hate', 'discrimination', 'slur', 'bigot',
        'supremacy', 'genocide'
    ],
    'manipulation_deception': [
        'manipulate', 'gaslight', 'deceive', 'trick', 'coerce', 'blackmail',
        'extort', 'threaten'
    ],
    'illegal_general': [
        'illegal', 'crime', 'criminal', 'unlawful', 'illicit', 'prohibited',
        'banned', 'contraband'
    ]
}

def categorize_prompt(prompt):
    """Assign categories to a prompt"""
    prompt_lower = prompt.lower()
    categories = []

    for category, keywords in harm_categories.items():
        for keyword in keywords:
            if keyword in prompt_lower:
                categories.append(category)
                break

    return categories if categories else ['other']

# ============================================================================
# VISUALIZE EACH MODEL
# ============================================================================

for model_name in MODELS_TO_VISUALIZE:

    print("\n" + "="*70)
    print(f"VISUALIZING: {model_name.upper()}")
    print("="*70)

    # Load model results
    pickle_path = f'{SAVE_DIR}/{model_name}_results.pkl'

    if not os.path.exists(pickle_path):
        print(f"✗ {model_name} not found, skipping...")
        continue

    with open(pickle_path, 'rb') as f:
        results = pickle.load(f)

    df = results['rankings'].copy()
    direction_info = results['direction_info']

    print(f"✓ Loaded {len(df)} ranked prompts")

    # Add categories
    df['categories'] = df['prompt'].apply(categorize_prompt)
    df['primary_category'] = df['categories'].apply(lambda x: x[0])
    df['n_categories'] = df['categories'].apply(len)

    # ========================================================================
    # CREATE COMPREHENSIVE VISUALIZATION
    # ========================================================================

    fig = plt.figure(figsize=(20, 16))
    gs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.3)

    # -----------------------------------------------------------------------
    # Plot 1: Score Distribution
    # -----------------------------------------------------------------------
    ax1 = fig.add_subplot(gs[0, 0])

    ax1.hist(df['refusal_score'], bins=50, color='steelblue',
            edgecolor='black', alpha=0.7)
    ax1.axvline(df['refusal_score'].mean(), color='red', linestyle='--',
               linewidth=2, label=f'Mean: {df["refusal_score"].mean():.2f}')
    ax1.axvline(df['refusal_score'].median(), color='orange', linestyle='--',
               linewidth=2, label=f'Median: {df["refusal_score"].median():.2f}')
    ax1.set_xlabel('Refusal Score', fontsize=11)
    ax1.set_ylabel('Count', fontsize=11)
    ax1.set_title('Refusal Score Distribution', fontsize=12, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # -----------------------------------------------------------------------
    # Plot 2: Rank Distribution
    # -----------------------------------------------------------------------
    ax2 = fig.add_subplot(gs[0, 1])

    ax2.hist(df['refusal_rank'], bins=50, color='coral',
            edgecolor='black', alpha=0.7)
    ax2.set_xlabel('Rank', fontsize=11)
    ax2.set_ylabel('Count', fontsize=11)
    ax2.set_title('Rank Distribution', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3)

    # -----------------------------------------------------------------------
    # Plot 3: Model Info Box
    # -----------------------------------------------------------------------
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.axis('off')

    info_text = f"""
    MODEL: {model_name.upper().replace('_', ' ')}

    Architecture: {results['model_path'].split('/')[-1]}
    Layers: {results['n_layers']}
    Hidden Size: {results['hidden_size']}

    REFUSAL DIRECTION:
    Layer: {direction_info['layer']}
    Magnitude: {direction_info['magnitude']:.3f}
    Cohen's d: {direction_info['cohens_d']:.3f}
    p-value: {direction_info['p_value']:.2e}
    Separation: {direction_info['separation']:.3f}

    RANKINGS:
    Total Prompts: {len(df)}
    Score Range: [{df['refusal_score'].min():.2f}, {df['refusal_score'].max():.2f}]
    Score Mean: {df['refusal_score'].mean():.2f}
    Score Std: {df['refusal_score'].std():.2f}
    """

    ax3.text(0.1, 0.5, info_text, fontsize=10, family='monospace',
            verticalalignment='center', bbox=dict(boxstyle='round',
            facecolor='wheat', alpha=0.3))

    # -----------------------------------------------------------------------
    # Plot 4: Top 20 Highest Scoring Prompts
    # -----------------------------------------------------------------------
    ax4 = fig.add_subplot(gs[1, :])

    top_20 = df.nsmallest(20, 'refusal_rank')

    y_positions = range(len(top_20))
    colors = plt.cm.Reds(np.linspace(0.4, 0.9, len(top_20)))

    ax4.barh(y_positions, top_20['refusal_score'], color=colors, alpha=0.8)
    ax4.set_yticks(y_positions)
    ax4.set_yticklabels([f"#{int(r)}" for r in top_20['refusal_rank']], fontsize=9)
    ax4.set_xlabel('Refusal Score', fontsize=11)
    ax4.set_title('Top 20 Prompts by Refusal Score', fontsize=12, fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='x')
    ax4.invert_yaxis()

    # Add prompt snippets
    for i, (idx, row) in enumerate(top_20.iterrows()):
        snippet = row['prompt'][:60] + "..." if len(row['prompt']) > 60 else row['prompt']
        ax4.text(row['refusal_score'] + 1, i, snippet,
                va='center', fontsize=8, style='italic')

    # -----------------------------------------------------------------------
    # Plot 5: Category Distribution
    # -----------------------------------------------------------------------
    ax5 = fig.add_subplot(gs[2, 0])

    category_counts = Counter()
    for cats in df['categories']:
        category_counts.update(cats)

    top_categories = category_counts.most_common(10)

    ax5.barh(range(len(top_categories)),
            [c[1] for c in top_categories],
            color='seagreen', alpha=0.7, edgecolor='black')
    ax5.set_yticks(range(len(top_categories)))
    ax5.set_yticklabels([c[0].replace('_', ' ').title() for c in top_categories],
                       fontsize=9)
    ax5.set_xlabel('Count', fontsize=11)
    ax5.set_title('Harm Category Distribution', fontsize=12, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='x')
    ax5.invert_yaxis()

    # -----------------------------------------------------------------------
    # Plot 6: Score by Category
    # -----------------------------------------------------------------------
    ax6 = fig.add_subplot(gs[2, 1])

    category_scores = df.groupby('primary_category')['refusal_score'].mean().sort_values(ascending=False).head(12)

    colors_cat = plt.cm.RdYlGn_r(np.linspace(0.3, 0.8, len(category_scores)))
    ax6.barh(range(len(category_scores)), category_scores.values,
            color=colors_cat, alpha=0.8, edgecolor='black')
    ax6.set_yticks(range(len(category_scores)))
    ax6.set_yticklabels([c.replace('_', ' ').title() for c in category_scores.index],
                       fontsize=9)
    ax6.set_xlabel('Mean Refusal Score', fontsize=11)
    ax6.set_title('Mean Score by Category', fontsize=12, fontweight='bold')
    ax6.grid(True, alpha=0.3, axis='x')
    ax6.invert_yaxis()

    # -----------------------------------------------------------------------
    # Plot 7: Score vs Rank
    # -----------------------------------------------------------------------
    ax7 = fig.add_subplot(gs[2, 2])

    # Sample for visualization
    sample_df = df.sample(min(500, len(df)))
    scatter = ax7.scatter(sample_df['refusal_rank'],
                         sample_df['refusal_score'],
                         c=sample_df['refusal_score'],
                         cmap='RdYlGn_r', alpha=0.5, s=20)
    ax7.set_xlabel('Rank', fontsize=11)
    ax7.set_ylabel('Refusal Score', fontsize=11)
    ax7.set_title('Score vs Rank', fontsize=12, fontweight='bold')
    ax7.grid(True, alpha=0.3)
    plt.colorbar(scatter, ax=ax7, label='Score')

    # -----------------------------------------------------------------------
    # Plot 8: Bottom 20 Lowest Scoring Prompts
    # -----------------------------------------------------------------------
    ax8 = fig.add_subplot(gs[3, :])

    bottom_20 = df.nlargest(20, 'refusal_rank')

    y_positions = range(len(bottom_20))
    colors_bottom = plt.cm.Greens(np.linspace(0.4, 0.9, len(bottom_20)))

    ax8.barh(y_positions, bottom_20['refusal_score'], color=colors_bottom, alpha=0.8)
    ax8.set_yticks(y_positions)
    ax8.set_yticklabels([f"#{int(r)}" for r in bottom_20['refusal_rank']], fontsize=9)
    ax8.set_xlabel('Refusal Score', fontsize=11)
    ax8.set_title('Bottom 20 Prompts by Refusal Score (Least Concerning)',
                 fontsize=12, fontweight='bold')
    ax8.grid(True, alpha=0.3, axis='x')
    ax8.invert_yaxis()

    # Add prompt snippets
    for i, (idx, row) in enumerate(bottom_20.iterrows()):
        snippet = row['prompt'][:60] + "..." if len(row['prompt']) > 60 else row['prompt']
        ax8.text(row['refusal_score'] + 0.5, i, snippet,
                va='center', fontsize=8, style='italic')

    # -----------------------------------------------------------------------
    # Save figure
    # -----------------------------------------------------------------------
    plt.suptitle(f'{model_name.upper().replace("_", " ")} - Refusal Analysis Dashboard',
                fontsize=18, fontweight='bold', y=0.995)

    fig_path = f'{SAVE_DIR}/{model_name}_dashboard.png'
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.show()

    print(f"✓ Dashboard saved: {fig_path}")

    # ========================================================================
    # GENERATE TEXT REPORT
    # ========================================================================

    print(f"\n{'='*70}")
    print(f"TEXT REPORT: {model_name.upper()}")
    print(f"{'='*70}")

    print(f"\n📊 SCORE STATISTICS:")
    print(f"   Mean: {df['refusal_score'].mean():.4f}")
    print(f"   Median: {df['refusal_score'].median():.4f}")
    print(f"   Std: {df['refusal_score'].std():.4f}")
    print(f"   Min: {df['refusal_score'].min():.4f}")
    print(f"   Max: {df['refusal_score'].max():.4f}")

    print(f"\n🔝 TOP 10 MOST CONCERNING PROMPTS:")
    for i, (idx, row) in enumerate(df.nsmallest(10, 'refusal_rank').iterrows(), 1):
        print(f"\n{i}. Rank #{int(row['refusal_rank'])} | Score: {row['refusal_score']:.4f}")
        print(f"   Categories: {', '.join(row['categories'])}")
        print(f"   Prompt: {row['prompt'][:150]}...")

    print(f"\n⬇️  TOP 10 LEAST CONCERNING PROMPTS:")
    for i, (idx, row) in enumerate(df.nlargest(10, 'refusal_rank').iterrows(), 1):
        print(f"\n{i}. Rank #{int(row['refusal_rank'])} | Score: {row['refusal_score']:.4f}")
        print(f"   Categories: {', '.join(row['categories'])}")
        print(f"   Prompt: {row['prompt'][:150]}...")

    print(f"\n📁 CATEGORY BREAKDOWN:")
    print(f"   Total categories found: {len(category_counts)}")
    print(f"\n   Top 10 categories:")
    for i, (cat, count) in enumerate(top_categories, 1):
        pct = count / len(df) * 100
        mean_score = df[df['categories'].apply(lambda x: cat in x)]['refusal_score'].mean()
        print(f"   {i}. {cat.replace('_', ' ').title()}: {count} ({pct:.1f}%) | Avg score: {mean_score:.3f}")

    # Save text report
    report_path = f'{SAVE_DIR}/{model_name}_report.txt'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"{'='*70}\n")
        f.write(f"REFUSAL ANALYSIS REPORT: {model_name.upper()}\n")
        f.write(f"{'='*70}\n\n")

        f.write(f"MODEL INFORMATION:\n")
        f.write(f"  Path: {results['model_path']}\n")
        f.write(f"  Layers: {results['n_layers']}\n")
        f.write(f"  Hidden Size: {results['hidden_size']}\n\n")

        f.write(f"REFUSAL DIRECTION:\n")
        f.write(f"  Cohen's d: {direction_info['cohens_d']:.3f}\n")
        f.write(f"  Magnitude: {direction_info['magnitude']:.3f}\n")
        f.write(f"  Separation: {direction_info['separation']:.3f}\n")
        f.write(f"  p-value: {direction_info['p_value']:.2e}\n\n")

        f.write(f"SCORE STATISTICS:\n")
        f.write(f"  Mean: {df['refusal_score'].mean():.4f}\n")
        f.write(f"  Median: {df['refusal_score'].median():.4f}\n")
        f.write(f"  Std: {df['refusal_score'].std():.4f}\n")
        f.write(f"  Range: [{df['refusal_score'].min():.4f}, {df['refusal_score'].max():.4f}]\n\n")

        f.write(f"TOP 20 MOST CONCERNING PROMPTS:\n")
        for i, (idx, row) in enumerate(df.nsmallest(20, 'refusal_rank').iterrows(), 1):
            f.write(f"\n{i}. Rank #{int(row['refusal_rank'])} | Score: {row['refusal_score']:.4f}\n")
            f.write(f"   Categories: {', '.join(row['categories'])}\n")
            f.write(f"   {row['prompt']}\n")

        f.write(f"\n\nCATEGORY ANALYSIS:\n")
        for i, (cat, count) in enumerate(top_categories, 1):
            pct = count / len(df) * 100
            mean_score = df[df['categories'].apply(lambda x: cat in x)]['refusal_score'].mean()
            f.write(f"{i}. {cat}: {count} ({pct:.1f}%) | Avg score: {mean_score:.3f}\n")

    print(f"\n✓ Text report saved: {report_path}")

    print(f"\n✓ {model_name.upper()} COMPLETE!")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "="*70)
print("✓ ALL VISUALIZATIONS COMPLETE!")
print("="*70)
print(f"\nGenerated for models:")
for model in MODELS_TO_VISUALIZE:
    print(f"  • {model}")
    print(f"    - Dashboard: {SAVE_DIR}/{model}_dashboard.png")
    print(f"    - Report: {SAVE_DIR}/{model}_report.txt")

print(f"\nAll files saved to: {SAVE_DIR}")

"""
STEP 3 REVISED: Better Individual Model Visualizations
=======================================================
Clearer, more readable visualizations for each model
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
from collections import Counter

SAVE_DIR = '/content/drive/MyDrive/LLM-refusal-directions-and-preferences/model_comparison'
MODELS_TO_VISUALIZE = ['qwen_1.5b', 'phi_3.5', 'phi_2.7b']

# Set better style
sns.set_style("whitegrid")
plt.rcParams['figure.facecolor'] = 'white'

# ============================================================================
# HARM CATEGORIES
# ============================================================================

harm_categories = {
    'violence_physical': [
        'kill', 'murder', 'assassinate', 'shoot', 'stab', 'torture', 'hurt',
        'attack', 'beating', 'poison', 'strangle', 'fight', 'assault', 'harm'
    ],
    'weapons_explosives': [
        'bomb', 'explosive', 'weapon', 'gun', 'knife', 'grenade', 'ammunition',
        'firearm', 'detonate', 'blast', 'missile'
    ],
    'illegal_drugs': [
        'drug', 'cocaine', 'heroin', 'meth', 'methamphetamine', 'narcotic',
        'trafficking', 'dealer', 'smuggle', 'cannabis'
    ],
    'child_harm': [
        'child', 'children', 'kid', 'minor', 'underage', 'baby', 'infant',
        'teenager', 'youth'
    ],
    'sexual_content': [
        'sex', 'sexual', 'rape', 'assault', 'abuse', 'molest', 'pornography'
    ],
    'theft_fraud': [
        'steal', 'theft', 'rob', 'fraud', 'scam', 'embezzle', 'counterfeit',
        'forge', 'hack'
    ],
    'suicide_selfharm': [
        'suicide', 'kill myself', 'self-harm', 'cutting', 'overdose'
    ],
    'hate_discrimination': [
        'racist', 'sexist', 'hate', 'discrimination', 'slur', 'bigot'
    ],
    'manipulation': [
        'manipulate', 'gaslight', 'deceive', 'trick', 'coerce', 'blackmail'
    ],
    'illegal_general': [
        'illegal', 'crime', 'criminal', 'unlawful', 'banned'
    ]
}

def categorize_prompt(prompt):
    prompt_lower = prompt.lower()
    categories = []
    for category, keywords in harm_categories.items():
        for keyword in keywords:
            if keyword in prompt_lower:
                categories.append(category)
                break
    return categories if categories else ['other']

# ============================================================================
# VISUALIZE EACH MODEL
# ============================================================================

for model_name in MODELS_TO_VISUALIZE:

    print("\n" + "="*70)
    print(f"VISUALIZING: {model_name.upper()}")
    print("="*70)

    # Load data
    pickle_path = f'{SAVE_DIR}/{model_name}_results.pkl'
    if not os.path.exists(pickle_path):
        print(f"✗ {model_name} not found")
        continue

    with open(pickle_path, 'rb') as f:
        results = pickle.load(f)

    df = results['rankings'].copy()
    direction_info = results['direction_info']

    # Add categories
    df['categories'] = df['prompt'].apply(categorize_prompt)
    df['primary_category'] = df['categories'].apply(lambda x: x[0])

    print(f"✓ Loaded {len(df)} prompts")

    # ========================================================================
    # MAIN DASHBOARD - REVISED
    # ========================================================================

    fig = plt.figure(figsize=(20, 14))
    gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)

    # ------------------------------------------------------------------------
    # Plot 1: Score Distribution with Statistics
    # ------------------------------------------------------------------------
    ax1 = fig.add_subplot(gs[0, 0])

    ax1.hist(df['refusal_score'], bins=50, color='steelblue',
            edgecolor='black', alpha=0.7)
    ax1.axvline(df['refusal_score'].mean(), color='red', linestyle='--',
               linewidth=2, label=f'Mean: {df["refusal_score"].mean():.2f}')
    ax1.axvline(df['refusal_score'].median(), color='orange', linestyle='--',
               linewidth=2, label=f'Median: {df["refusal_score"].median():.2f}')

    # Add percentiles
    p25 = df['refusal_score'].quantile(0.25)
    p75 = df['refusal_score'].quantile(0.75)
    ax1.axvline(p25, color='green', linestyle=':', linewidth=1.5, alpha=0.7)
    ax1.axvline(p75, color='green', linestyle=':', linewidth=1.5, alpha=0.7)

    ax1.set_xlabel('Refusal Score', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax1.set_title('Score Distribution', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)

    # ------------------------------------------------------------------------
    # Plot 2: Model Statistics Box
    # ------------------------------------------------------------------------
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.axis('off')

    stats_text = f"""
MODEL: {model_name.upper().replace('_', ' ')}

ARCHITECTURE:
  • Layers: {results['n_layers']}
  • Hidden Size: {results['hidden_size']}

REFUSAL DIRECTION:
  • Cohen's d: {direction_info['cohens_d']:.3f}
  • Magnitude: {direction_info['magnitude']:.3f}
  • Separation: {direction_info['separation']:.3f}
  • p-value: {direction_info['p_value']:.2e}

SCORE STATISTICS:
  • Mean: {df['refusal_score'].mean():.3f}
  • Median: {df['refusal_score'].median():.3f}
  • Std Dev: {df['refusal_score'].std():.3f}
  • Range: [{df['refusal_score'].min():.2f}, {df['refusal_score'].max():.2f}]
  • IQR: [{p25:.2f}, {p75:.2f}]

DATASET:
  • Total Prompts: {len(df)}
  • Categories: {len(set(df['primary_category']))}
    """

    ax2.text(0.05, 0.5, stats_text, fontsize=11, family='monospace',
            verticalalignment='center',
            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.2, pad=1))

    # ------------------------------------------------------------------------
    # Plot 3: Category Distribution (Pie Chart)
    # ------------------------------------------------------------------------
    ax3 = fig.add_subplot(gs[0, 2])

    category_counts = Counter()
    for cats in df['categories']:
        category_counts.update(cats)

    top_cats = dict(category_counts.most_common(8))
    other_count = sum(category_counts.values()) - sum(top_cats.values())
    if other_count > 0:
        top_cats['other'] = other_count

    colors = plt.cm.Set3(np.linspace(0, 1, len(top_cats)))
    wedges, texts, autotexts = ax3.pie(top_cats.values(),
                                        labels=[c.replace('_', '\n').title() for c in top_cats.keys()],
                                        autopct='%1.1f%%',
                                        colors=colors,
                                        startangle=90)

    for text in texts:
        text.set_fontsize(9)
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(9)

    ax3.set_title('Harm Category Distribution', fontsize=14, fontweight='bold')

    # ------------------------------------------------------------------------
    # Plot 4: Top 10 Categories by Mean Score
    # ------------------------------------------------------------------------
    ax4 = fig.add_subplot(gs[1, :])

    category_scores = df.groupby('primary_category')['refusal_score'].agg(['mean', 'count']).sort_values('mean', ascending=False).head(10)

    x = np.arange(len(category_scores))
    colors = plt.cm.RdYlGn_r(np.linspace(0.3, 0.8, len(category_scores)))

    bars = ax4.bar(x, category_scores['mean'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)

    # Add count labels on bars
    for i, (idx, row) in enumerate(category_scores.iterrows()):
        ax4.text(i, row['mean'] + 1, f"n={int(row['count'])}",
                ha='center', va='bottom', fontsize=9, fontweight='bold')

    ax4.set_xticks(x)
    ax4.set_xticklabels([c.replace('_', '\n').title() for c in category_scores.index],
                       fontsize=10, fontweight='bold')
    ax4.set_ylabel('Mean Refusal Score', fontsize=12, fontweight='bold')
    ax4.set_title('Top 10 Categories by Mean Refusal Score', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='y')

    # Add horizontal line for overall mean
    ax4.axhline(df['refusal_score'].mean(), color='red', linestyle='--',
               linewidth=2, alpha=0.7, label=f'Overall Mean: {df["refusal_score"].mean():.2f}')
    ax4.legend(fontsize=10)

    # ------------------------------------------------------------------------
    # Plot 5: Score Ranges by Category (Box Plot)
    # ------------------------------------------------------------------------
    ax5 = fig.add_subplot(gs[2, 0])

    top_5_cats = category_scores.head(5).index.tolist()
    data_for_box = [df[df['primary_category'] == cat]['refusal_score'].values for cat in top_5_cats]

    bp = ax5.boxplot(data_for_box, labels=[c.replace('_', '\n').title() for c in top_5_cats],
                     patch_artist=True, showmeans=True)

    for patch, color in zip(bp['boxes'], plt.cm.Set2(np.linspace(0, 1, 5))):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax5.set_ylabel('Refusal Score', fontsize=11, fontweight='bold')
    ax5.set_title('Score Distribution by Top 5 Categories', fontsize=12, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')
    plt.setp(ax5.get_xticklabels(), fontsize=9)

    # ------------------------------------------------------------------------
    # Plot 6: Rank Percentiles
    # ------------------------------------------------------------------------
    ax6 = fig.add_subplot(gs[2, 1])

    percentiles = [10, 20, 30, 40, 50, 60, 70, 80, 90]
    percentile_scores = [df['refusal_score'].quantile(p/100) for p in percentiles]

    ax6.plot(percentiles, percentile_scores, 'o-', linewidth=3, markersize=8, color='darkblue')
    ax6.fill_between(percentiles, percentile_scores, alpha=0.3, color='lightblue')

    ax6.set_xlabel('Percentile', fontsize=11, fontweight='bold')
    ax6.set_ylabel('Refusal Score', fontsize=11, fontweight='bold')
    ax6.set_title('Score Percentiles', fontsize=12, fontweight='bold')
    ax6.grid(True, alpha=0.3)

    # Annotate key percentiles
    for p, s in [(25, df['refusal_score'].quantile(0.25)),
                 (50, df['refusal_score'].median()),
                 (75, df['refusal_score'].quantile(0.75))]:
        if p in percentiles:
            idx = percentiles.index(p)
            ax6.annotate(f'{s:.1f}', xy=(p, s), xytext=(p, s+3),
                        fontsize=9, ha='center', fontweight='bold',
                        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))

    # ------------------------------------------------------------------------
    # Plot 7: Cumulative Distribution
    # ------------------------------------------------------------------------
    ax7 = fig.add_subplot(gs[2, 2])

    sorted_scores = np.sort(df['refusal_score'])
    cumulative = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores) * 100

    ax7.plot(sorted_scores, cumulative, linewidth=2.5, color='darkgreen')
    ax7.fill_between(sorted_scores, cumulative, alpha=0.3, color='lightgreen')

    # Add reference lines
    median_score = df['refusal_score'].median()
    ax7.axvline(median_score, color='red', linestyle='--', linewidth=2,
               label=f'Median: {median_score:.2f}')
    ax7.axhline(50, color='red', linestyle='--', linewidth=1, alpha=0.5)

    ax7.set_xlabel('Refusal Score', fontsize=11, fontweight='bold')
    ax7.set_ylabel('Cumulative %', fontsize=11, fontweight='bold')
    ax7.set_title('Cumulative Distribution', fontsize=12, fontweight='bold')
    ax7.legend(fontsize=9)
    ax7.grid(True, alpha=0.3)

    # ------------------------------------------------------------------------
    # Save main dashboard
    # ------------------------------------------------------------------------
    plt.suptitle(f'{model_name.upper().replace("_", " ")} - Refusal Analysis Dashboard',
                fontsize=18, fontweight='bold', y=0.998)

    fig_path = f'{SAVE_DIR}/{model_name}_dashboard_v2.png'
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"✓ Dashboard saved: {fig_path}")

    # ========================================================================
    # SEPARATE FIGURE: TOP/BOTTOM PROMPTS TABLE
    # ========================================================================

    fig2, axes = plt.subplots(2, 1, figsize=(18, 12))

    # Top 10 prompts
    ax_top = axes[0]
    ax_top.axis('tight')
    ax_top.axis('off')

    top_10 = df.nsmallest(10, 'refusal_rank')

    table_data_top = []
    for idx, row in top_10.iterrows():
        rank = int(row['refusal_rank'])
        score = f"{row['refusal_score']:.3f}"
        categories = ', '.join(row['categories'][:2])  # Show max 2 categories
        prompt = row['prompt'][:90] + "..." if len(row['prompt']) > 90 else row['prompt']
        table_data_top.append([rank, score, categories, prompt])

    table_top = ax_top.table(cellText=table_data_top,
                            colLabels=['Rank', 'Score', 'Categories', 'Prompt'],
                            cellLoc='left',
                            colWidths=[0.06, 0.08, 0.18, 0.68],
                            loc='center',
                            bbox=[0, 0, 1, 1])

    table_top.auto_set_font_size(False)
    table_top.set_fontsize(10)
    table_top.scale(1, 2.5)

    # Color code by rank
    for i in range(len(table_data_top)):
        color = plt.cm.Reds(0.3 + (i / len(table_data_top)) * 0.5)
        for j in range(4):
            table_top[(i+1, j)].set_facecolor(color)
            table_top[(i+1, j)].set_alpha(0.3)

    # Style header
    for j in range(4):
        table_top[(0, j)].set_facecolor('darkblue')
        table_top[(0, j)].set_text_props(weight='bold', color='white')

    ax_top.set_title(f'Top 10 Most Concerning Prompts ({model_name.upper()})',
                    fontsize=14, fontweight='bold', pad=20)

    # Bottom 10 prompts
    ax_bottom = axes[1]
    ax_bottom.axis('tight')
    ax_bottom.axis('off')

    bottom_10 = df.nlargest(10, 'refusal_rank')

    table_data_bottom = []
    for idx, row in bottom_10.iterrows():
        rank = int(row['refusal_rank'])
        score = f"{row['refusal_score']:.3f}"
        categories = ', '.join(row['categories'][:2])
        prompt = row['prompt'][:90] + "..." if len(row['prompt']) > 90 else row['prompt']
        table_data_bottom.append([rank, score, categories, prompt])

    table_bottom = ax_bottom.table(cellText=table_data_bottom,
                                   colLabels=['Rank', 'Score', 'Categories', 'Prompt'],
                                   cellLoc='left',
                                   colWidths=[0.06, 0.08, 0.18, 0.68],
                                   loc='center',
                                   bbox=[0, 0, 1, 1])

    table_bottom.auto_set_font_size(False)
    table_bottom.set_fontsize(10)
    table_bottom.scale(1, 2.5)

    # Color code
    for i in range(len(table_data_bottom)):
        color = plt.cm.Greens(0.3 + (i / len(table_data_bottom)) * 0.4)
        for j in range(4):
            table_bottom[(i+1, j)].set_facecolor(color)
            table_bottom[(i+1, j)].set_alpha(0.3)

    # Style header
    for j in range(4):
        table_bottom[(0, j)].set_facecolor('darkgreen')
        table_bottom[(0, j)].set_text_props(weight='bold', color='white')

    ax_bottom.set_title(f'Top 10 Least Concerning Prompts ({model_name.upper()})',
                       fontsize=14, fontweight='bold', pad=20)

    plt.tight_layout()
    table_path = f'{SAVE_DIR}/{model_name}_prompts_table.png'
    plt.savefig(table_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"✓ Prompts table saved: {table_path}")

    print(f"\n✓ {model_name.upper()} VISUALIZATION COMPLETE!\n")

print("\n" + "="*70)
print("✓ ALL VISUALIZATIONS COMPLETE!")
print("="*70)

"""
QWEN 3B & 7B - REFUSAL ANALYSIS DASHBOARD
==========================================
Complete analysis pipeline for both models
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Configuration
BASE_DIR = '/content/drive/MyDrive/LLM-refusal-directions-and-preferences/model_comparison'

# Models to process
MODELS = [
    {
        'name': 'qwen_3b',
        'display_name': 'QWEN 3B',
        'file': 'harmful_projections_last_layer_from_pickle_Qwen3b.csv',
        'layers': 'TBD',
        'hidden_size': 'TBD'
    },
    {
        'name': 'qwen_7b',
        'display_name': 'QWEN 7B',
        'file': 'prompt_projections_lastlayer_Qwen7b.csv',
        'layers': 'TBD',
        'hidden_size': 'TBD'
    }
]

# ============================================================================
# HARM CATEGORIES DEFINITION
# ============================================================================

harm_categories = {
    'violence_physical': [
        'kill', 'murder', 'assassinate', 'shoot', 'stab', 'torture', 'hurt',
        'attack', 'beating', 'poison', 'strangle', 'drown', 'suffocate', 'punch',
        'fight', 'assault', 'harm', 'injure', 'wound', 'maim'
    ],
    'weapons_explosives': [
        'bomb', 'explosive', 'weapon', 'gun', 'knife', 'grenade', 'ammunition',
        'firearm', 'detonate', 'blast', 'missile', 'dynamite', 'c4', 'ied',
        'rifle', 'pistol', 'molotov'
    ],
    'illegal_drugs': [
        'drug', 'cocaine', 'heroin', 'meth', 'methamphetamine', 'narcotic',
        'trafficking', 'dealer', 'smuggle', 'cannabis', 'fentanyl', 'mdma',
        'ecstasy', 'lsd', 'marijuana', 'opioid', 'cartel'
    ],
    'child_harm': [
        'child', 'children', 'kid', 'minor', 'underage', 'baby', 'infant',
        'teenager', 'youth', 'juvenile', 'toddler', 'adolescent', 'schoolchild'
    ],
    'suicide_selfharm': [
        'suicide', 'self-harm', 'self harm', 'selfharm', 'kill yourself',
        'end your life', 'take your own life', 'cut yourself', 'hurt yourself'
    ],
    'theft_fraud': [
        'steal', 'theft', 'fraud', 'scam', 'embezzle', 'counterfeit', 'forge',
        'cheat', 'swindle', 'con', 'phishing', 'identity theft', 'credit card fraud',
        'insider trading', 'tax evasion', 'money laundering'
    ],
    'hate_discrimination': [
        'hate', 'discrimination', 'racist', 'racism', 'sexist', 'sexism',
        'homophobic', 'transphobic', 'antisemitic', 'islamophobic', 'bigot',
        'prejudice', 'slur', 'derogatory'
    ],
    'manipulation': [
        'manipulate', 'deceive', 'trick', 'coerce', 'blackmail', 'extort',
        'gaslight', 'brainwash', 'exploit', 'abuse'
    ],
    'illegal_general': [
        'illegal', 'unlawful', 'criminal', 'illicit', 'contraband', 'smuggle',
        'traffic', 'launder'
    ],
    'other': []
}

def categorize_prompt(prompt_text):
    """Categorize prompt into harm categories"""
    prompt_lower = prompt_text.lower()
    categories = []

    for category, keywords in harm_categories.items():
        if category == 'other':
            continue
        if any(keyword in prompt_lower for keyword in keywords):
            categories.append(category)

    if not categories:
        categories.append('other')

    return categories

# ============================================================================
# MAIN PROCESSING LOOP
# ============================================================================

for model_config in MODELS:

    model_name = model_config['name']
    display_name = model_config['display_name']
    file_name = model_config['file']

    print("\n" + "="*70)
    print(f"{display_name} - REFUSAL ANALYSIS DASHBOARD")
    print("="*70)

    # ========================================================================
    # STEP 1: LOAD DATA
    # ========================================================================

    print(f"\n[1/7] Loading {display_name} dataset...")

    file_path = f'{BASE_DIR}/{file_name}'
    df = pd.read_csv(file_path)

    print(f"✓ Loaded {len(df)} prompts")
    print(f"  Columns: {df.columns.tolist()}")
    print(f"  First few rows:")
    print(df.head(3))

    # Standardize column names
    if 'projection_last_layer' in df.columns:
        df = df.rename(columns={'projection_last_layer': 'refusal_score'})
    elif 'refusal_score' not in df.columns:
        # Try to find the score column
        score_cols = [col for col in df.columns if 'score' in col.lower() or 'projection' in col.lower()]
        if score_cols:
            df = df.rename(columns={score_cols[0]: 'refusal_score'})
            print(f"  Renamed '{score_cols[0]}' to 'refusal_score'")

    # Check if 'split' column exists, if not, create it or handle differently
    if 'split' not in df.columns:
        print(f"  WARNING: 'split' column not found. Checking for alternatives...")
        # Check if there's a column that indicates harmful/harmless
        if 'label' in df.columns:
            df = df.rename(columns={'label': 'split'})
            print(f"  Renamed 'label' to 'split'")
        elif 'type' in df.columns:
            df = df.rename(columns={'type': 'split'})
            print(f"  Renamed 'type' to 'split'")
        else:
            # If no split column exists, assume all are harmful
            print(f"  No split column found. Assuming all prompts are harmful.")
            df['split'] = 'harmful'

    # ========================================================================
    # STEP 2: SEPARATE HARMFUL/HARMLESS
    # ========================================================================

    print(f"\n[2/7] Separating harmful and harmless prompts...")

    df_harmful = df[df['split'] == 'harmful'].copy()
    df_harmless = df[df['split'] == 'harmless'].copy()

    print(f"  Harmful prompts: {len(df_harmful)}")
    print(f"  Harmless prompts: {len(df_harmless)}")

    # If no harmless prompts, we can't calculate Cohen's d properly
    if len(df_harmless) == 0:
        print(f"  WARNING: No harmless prompts found. Creating dummy harmless set for statistics.")
        # We'll skip the Cohen's d calculation or use a placeholder
        has_harmless = False
    else:
        has_harmless = True

    # ========================================================================
    # STEP 3: CALCULATE STATISTICS
    # ========================================================================

    print(f"\n[3/7] Calculating refusal direction statistics...")

    if has_harmless:
        harmful_mean = df_harmful['refusal_score'].mean()
        harmless_mean = df_harmless['refusal_score'].mean()
        harmful_std = df_harmful['refusal_score'].std()
        harmless_std = df_harmless['refusal_score'].std()

        # Cohen's d
        pooled_std = np.sqrt(((len(df_harmful) - 1) * harmful_std**2 +
                               (len(df_harmless) - 1) * harmless_std**2) /
                              (len(df_harmful) + len(df_harmless) - 2))
        cohens_d = (harmful_mean - harmless_mean) / pooled_std

        # Magnitude and separation
        magnitude = abs(harmful_mean - harmless_mean)
        separation = magnitude

        # Statistical test
        t_stat, p_value = stats.ttest_ind(df_harmful['refusal_score'],
                                           df_harmless['refusal_score'])
    else:
        # No harmless prompts - use placeholders
        cohens_d = np.nan
        magnitude = np.nan
        separation = np.nan
        p_value = np.nan

    print(f"\n  REFUSAL DIRECTION METRICS:")
    if has_harmless:
        print(f"  Cohen's d: {cohens_d:.3f}")
        print(f"  Magnitude: {magnitude:.3f}")
        print(f"  Separation: {separation:.3f}")
        print(f"  p-value: {p_value:.3e}")
    else:
        print(f"  Cohen's d: N/A (no harmless prompts)")
        print(f"  Magnitude: N/A")
        print(f"  Separation: N/A")
        print(f"  p-value: N/A")

    # ========================================================================
    # STEP 4: CATEGORIZE HARMFUL PROMPTS
    # ========================================================================

    print(f"\n[4/7] Categorizing harmful prompts...")

    df_harmful['categories'] = df_harmful['prompt'].apply(categorize_prompt)
    df_harmful['primary_category'] = df_harmful['categories'].apply(lambda x: x[0])

    category_counts = df_harmful['primary_category'].value_counts()
    category_scores = df_harmful.groupby('primary_category')['refusal_score'].agg(['mean', 'std', 'count'])
    category_scores = category_scores.sort_values('mean', ascending=False)

    print(f"\n  Categories identified: {len(category_counts)}")
    print(f"\n  Top 5 categories by mean score:")
    for idx, row in category_scores.head(5).iterrows():
        print(f"    {idx.replace('_', ' ').title()}: {row['mean']:.2f} (n={int(row['count'])})")

    # ========================================================================
    # STEP 5: SCORE STATISTICS
    # ========================================================================

    print(f"\n[5/7] Computing score statistics...")

    score_stats = {
        'Mean': df_harmful['refusal_score'].mean(),
        'Median': df_harmful['refusal_score'].median(),
        'Std Dev': df_harmful['refusal_score'].std(),
        'Min': df_harmful['refusal_score'].min(),
        'Max': df_harmful['refusal_score'].max(),
        'IQR': df_harmful['refusal_score'].quantile(0.75) - df_harmful['refusal_score'].quantile(0.25),
        'Q1': df_harmful['refusal_score'].quantile(0.25),
        'Q3': df_harmful['refusal_score'].quantile(0.75)
    }

    print("\n  SCORE STATISTICS:")
    for key, value in score_stats.items():
        print(f"    {key}: {value:.2f}")

    # Rank prompts
    df_harmful['refusal_rank'] = df_harmful['refusal_score'].rank(ascending=False, method='min')
    df_harmful = df_harmful.sort_values('refusal_rank')

    # ========================================================================
    # STEP 6: CREATE DASHBOARD
    # ========================================================================

    print(f"\n[6/7] Creating visualization dashboard...")

    # Set style
    plt.style.use('default')
    sns.set_palette("husl")

    # Create main figure
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)

    # ====================================================================
    # Plot 1: Score Distribution Histogram
    # ====================================================================
    ax1 = fig.add_subplot(gs[0, 0])

    ax1.hist(df_harmful['refusal_score'], bins=50, alpha=0.7, color='steelblue', edgecolor='black')
    ax1.axvline(score_stats['Mean'], color='red', linestyle='--', linewidth=2,
                label=f"Mean: {score_stats['Mean']:.2f}")
    ax1.axvline(score_stats['Median'], color='orange', linestyle='--', linewidth=2,
                label=f"Median: {score_stats['Median']:.2f}")

    ax1.set_xlabel('Refusal Score', fontsize=11, fontweight='bold')
    ax1.set_ylabel('Frequency', fontsize=11, fontweight='bold')
    ax1.set_title('Score Distribution', fontsize=12, fontweight='bold')
    ax1.legend(fontsize=9)
    ax1.grid(True, alpha=0.3, axis='y')

    # ====================================================================
    # Plot 2: Model Info Box
    # ====================================================================
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.axis('off')

    if has_harmless:
        refusal_info = f"""• Cohen's d: {cohens_d:.3f}
• Magnitude: {magnitude:.3f}
• Separation: {separation:.3f}
• p-value: {p_value:.2e}"""
    else:
        refusal_info = """• Cohen's d: N/A
• Magnitude: N/A
• Separation: N/A
• p-value: N/A"""

    info_text = f"""MODEL: {display_name}

ARCHITECTURE:
• Layers: {model_config['layers']}
• Hidden Size: {model_config['hidden_size']}

REFUSAL DIRECTION:
{refusal_info}

SCORE STATISTICS:
• Mean: {score_stats['Mean']:.3f}
• Median: {score_stats['Median']:.3f}
• Std Dev: {score_stats['Std Dev']:.3f}
• Range: [{score_stats['Min']:.2f}, {score_stats['Max']:.2f}]
• IQR: [{score_stats['Q1']:.2f}, {score_stats['Q3']:.2f}]

DATASET:
• Total Prompts: {len(df_harmful)}
• Categories: {len(category_counts)}"""

    ax2.text(0.1, 0.5, info_text, fontsize=10, family='monospace',
             verticalalignment='center',
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))

    # ====================================================================
    # Plot 3: Harm Category Distribution (Pie Chart)
    # ====================================================================
    ax3 = fig.add_subplot(gs[0, 2])

    category_percentages = (category_counts / category_counts.sum()) * 100
    colors = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))

    wedges, texts, autotexts = ax3.pie(category_counts,
                                         labels=[c.replace('_', '\n').title() for c in category_counts.index],
                                         autopct='%1.1f%%',
                                         colors=colors,
                                         startangle=90)

    for autotext in autotexts:
        autotext.set_color('black')
        autotext.set_fontsize(8)
        autotext.set_fontweight('bold')

    ax3.set_title('Harm Category Distribution', fontsize=12, fontweight='bold')

    # ====================================================================
    # Plot 4: Top 10 Categories by Mean Score
    # ====================================================================
    ax4 = fig.add_subplot(gs[1, :])

    top_10_cats = category_scores.head(10)
    x_pos = np.arange(len(top_10_cats))
    colors_bar = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(top_10_cats)))

    bars = ax4.bar(x_pos, top_10_cats['mean'], color=colors_bar, alpha=0.8, edgecolor='black')

    # Add count labels
    for i, (bar, count) in enumerate(zip(bars, top_10_cats['count'])):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                f'n={int(count)}',
                ha='center', va='bottom', fontsize=9, fontweight='bold')

    ax4.axhline(score_stats['Mean'], color='red', linestyle='--', linewidth=2,
               label=f"Overall Mean: {score_stats['Mean']:.2f}", alpha=0.7)

    ax4.set_xlabel('Category', fontsize=11, fontweight='bold')
    ax4.set_ylabel('Mean Refusal Score', fontsize=11, fontweight='bold')
    ax4.set_title('Top 10 Categories by Mean Refusal Score', fontsize=12, fontweight='bold')
    ax4.set_xticks(x_pos)
    ax4.set_xticklabels([c.replace('_', '\n').title() for c in top_10_cats.index], fontsize=9)
    ax4.legend(fontsize=9)
    ax4.grid(True, alpha=0.3, axis='y')

    # ====================================================================
    # Plot 5: Box Plot
    # ====================================================================
    ax5 = fig.add_subplot(gs[2, 0])

    top_5_cats = category_scores.head(5).index.tolist()
    data_for_box = [df_harmful[df_harmful['primary_category'] == cat]['refusal_score'].values
                    for cat in top_5_cats]

    bp = ax5.boxplot(data_for_box,
                     labels=[c.replace('_', '\n').title() for c in top_5_cats],
                     patch_artist=True,
                     showmeans=True)

    for patch, color in zip(bp['boxes'], plt.cm.Set2(np.linspace(0, 1, 5))):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax5.set_ylabel('Refusal Score', fontsize=11, fontweight='bold')
    ax5.set_title('Score Distribution by Top 5 Categories', fontsize=12, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')
    plt.setp(ax5.get_xticklabels(), fontsize=9)

    # ====================================================================
    # Plot 6: Percentiles
    # ====================================================================
    ax6 = fig.add_subplot(gs[2, 1])

    percentiles = [10, 20, 30, 40, 50, 60, 70, 80, 90]
    percentile_scores = [df_harmful['refusal_score'].quantile(p/100) for p in percentiles]

    ax6.plot(percentiles, percentile_scores, 'o-', linewidth=3, markersize=8, color='darkblue')
    ax6.fill_between(percentiles, percentile_scores, alpha=0.3, color='lightblue')

    ax6.set_xlabel('Percentile', fontsize=11, fontweight='bold')
    ax6.set_ylabel('Refusal Score', fontsize=11, fontweight='bold')
    ax6.set_title('Score Percentiles', fontsize=12, fontweight='bold')
    ax6.grid(True, alpha=0.3)

    # Annotate median
    median_val = score_stats['Median']
    ax6.annotate(f'{median_val:.1f}',
                xy=(50, median_val),
                xytext=(50, median_val+5),
                fontsize=9, ha='center', fontweight='bold',
                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))

    # ====================================================================
    # Plot 7: Cumulative Distribution
    # ====================================================================
    ax7 = fig.add_subplot(gs[2, 2])

    sorted_scores = np.sort(df_harmful['refusal_score'])
    cumulative = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores) * 100

    ax7.plot(sorted_scores, cumulative, linewidth=2.5, color='darkgreen')
    ax7.fill_between(sorted_scores, cumulative, alpha=0.3, color='lightgreen')

    ax7.axvline(score_stats['Median'], color='red', linestyle='--', linewidth=2,
               label=f'Median: {score_stats["Median"]:.2f}')
    ax7.axhline(50, color='red', linestyle='--', linewidth=1, alpha=0.5)

    ax7.set_xlabel('Refusal Score', fontsize=11, fontweight='bold')
    ax7.set_ylabel('Cumulative %', fontsize=11, fontweight='bold')
    ax7.set_title('Cumulative Distribution', fontsize=12, fontweight='bold')
    ax7.legend(fontsize=9)
    ax7.grid(True, alpha=0.3)

    # ====================================================================
    # Save Dashboard
    # ====================================================================
    plt.suptitle(f'{display_name} - Refusal Analysis Dashboard',
                fontsize=18, fontweight='bold', y=0.998)

    fig_path = f'{BASE_DIR}/{model_name}_dashboard.png'
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"✓ Dashboard saved: {fig_path}")

    plt.close()

    # ========================================================================
    # STEP 7: CREATE PROMPTS TABLE
    # ========================================================================

    print(f"\n[7/7] Creating prompts table...")

    fig2, axes = plt.subplots(2, 1, figsize=(18, 12))

    # Top 10
    ax_top = axes[0]
    ax_top.axis('tight')
    ax_top.axis('off')

    top_10 = df_harmful.head(10)

    table_data_top = []
    for idx, row in top_10.iterrows():
        rank = int(row['refusal_rank'])
        score = f"{row['refusal_score']:.3f}"
        categories = ', '.join(row['categories'][:2])
        prompt = row['prompt'][:85] + "..." if len(row['prompt']) > 85 else row['prompt']
        table_data_top.append([rank, score, categories, prompt])

    table_top = ax_top.table(cellText=table_data_top,
                            colLabels=['Rank', 'Score', 'Categories', 'Prompt'],
                            cellLoc='left',
                            colWidths=[0.06, 0.08, 0.18, 0.68],
                            loc='center',
                            bbox=[0, 0, 1, 1])

    table_top.auto_set_font_size(False)
    table_top.set_fontsize(10)
    table_top.scale(1, 2.5)

    for i in range(len(table_data_top)):
        color = plt.cm.Reds(0.3 + (i / len(table_data_top)) * 0.5)
        for j in range(4):
            table_top[(i+1, j)].set_facecolor(color)
            table_top[(i+1, j)].set_alpha(0.3)

    for j in range(4):
        table_top[(0, j)].set_facecolor('darkblue')
        table_top[(0, j)].set_text_props(weight='bold', color='white')

    ax_top.set_title(f'Top 10 Highest Refusal Score Prompts ({display_name})',
                    fontsize=14, fontweight='bold', pad=20)

    # Bottom 10
    ax_bottom = axes[1]
    ax_bottom.axis('tight')
    ax_bottom.axis('off')

    bottom_10 = df_harmful.tail(10)

    table_data_bottom = []
    for idx, row in bottom_10.iterrows():
        rank = int(row['refusal_rank'])
        score = f"{row['refusal_score']:.3f}"
        categories = ', '.join(row['categories'][:2])
        prompt = row['prompt'][:85] + "..." if len(row['prompt']) > 85 else row['prompt']
        table_data_bottom.append([rank, score, categories, prompt])

    table_bottom = ax_bottom.table(cellText=table_data_bottom,
                                   colLabels=['Rank', 'Score', 'Categories', 'Prompt'],
                                   cellLoc='left',
                                   colWidths=[0.06, 0.08, 0.18, 0.68],
                                   loc='center',
                                   bbox=[0, 0, 1, 1])

    table_bottom.auto_set_font_size(False)
    table_bottom.set_fontsize(10)
    table_bottom.scale(1, 2.5)

    for i in range(len(table_data_bottom)):
        color = plt.cm.Greens(0.3 + (i / len(table_data_bottom)) * 0.4)
        for j in range(4):
            table_bottom[(i+1, j)].set_facecolor(color)
            table_bottom[(i+1, j)].set_alpha(0.3)

    for j in range(4):
        table_bottom[(0, j)].set_facecolor('darkgreen')
        table_bottom[(0, j)].set_text_props(weight='bold', color='white')

    ax_bottom.set_title(f'Top 10 Lowest Refusal Score Prompts ({display_name})',
                       fontsize=14, fontweight='bold', pad=20)

    plt.tight_layout()
    table_path = f'{BASE_DIR}/{model_name}_prompts_table.png'
    plt.savefig(table_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"✓ Prompts table saved: {table_path}")

    plt.close()

    # ========================================================================
    # SAVE CSV RESULTS
    # ========================================================================

    # Ranked prompts
    df_harmful_export = df_harmful[['refusal_rank', 'refusal_score', 'primary_category', 'categories', 'prompt']].copy()
    df_harmful_export['categories'] = df_harmful_export['categories'].apply(lambda x: ', '.join(x))
    df_harmful_export.to_csv(f'{BASE_DIR}/{model_name}_ranked_prompts.csv', index=False)

    # Category stats
    category_scores.to_csv(f'{BASE_DIR}/{model_name}_category_stats.csv')

    # Summary stats
    summary_stats = pd.DataFrame({
        'Metric': ['Mean', 'Median', 'Std Dev', 'Min', 'Max', 'IQR', 'Q1', 'Q3',
                   'Cohens_d', 'Magnitude', 'Separation', 'P_value'],
        'Value': [score_stats['Mean'], score_stats['Median'], score_stats['Std Dev'],
                  score_stats['Min'], score_stats['Max'], score_stats['IQR'],
                  score_stats['Q1'], score_stats['Q3'],
                  cohens_d if has_harmless else np.nan,
                  magnitude if has_harmless else np.nan,
                  separation if has_harmless else np.nan,
                  p_value if has_harmless else np.nan]
    })
    summary_stats.to_csv(f'{BASE_DIR}/{model_name}_summary_stats.csv', index=False)

    print("\n" + "="*70)
    print(f"✓ {display_name} ANALYSIS COMPLETE!")
    print("="*70)
    print(f"\nKey Findings:")
    print(f"  • Mean Refusal Score: {score_stats['Mean']:.2f}")
    print(f"  • Median Refusal Score: {score_stats['Median']:.2f}")
    if has_harmless:
        print(f"  • Cohen's d: {cohens_d:.3f}")
    else:
        print(f"  • Cohen's d: N/A (no harmless prompts)")
    print(f"  • Top Category: {category_scores.index[0].replace('_', ' ').title()}")
    print(f"  • Total Harmful Prompts: {len(df_harmful)}")
    print("="*70)

print("\n\n" + "="*70)
print("✓✓✓ ALL MODELS PROCESSED SUCCESSFULLY ✓✓✓")
print("="*70)

"""
QWEN 3B & 7B - REFUSAL ANALYSIS DASHBOARD
==========================================
Complete analysis pipeline for both models
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Configuration
BASE_DIR = '/content/drive/MyDrive/LLM-refusal-directions-and-preferences/model_comparison'

# Models to process
MODELS = [
    {
        'name': 'qwen_3b',
        'display_name': 'QWEN 3B',
        'file': 'harmful_projections_last_layer_from_pickle_Qwen3b.csv',
        'layers': 'TBD',
        'hidden_size': 'TBD'
    },
    {
        'name': 'qwen_7b',
        'display_name': 'QWEN 7B',
        'file': 'prompt_projections_lastlayer_Qwen7b.csv',
        'layers': 'TBD',
        'hidden_size': 'TBD'
    }
]

# ============================================================================
# HARM CATEGORIES DEFINITION
# ============================================================================

harm_categories = {
    'violence_physical': [
        'kill', 'murder', 'assassinate', 'shoot', 'stab', 'torture', 'hurt',
        'attack', 'beating', 'poison', 'strangle', 'drown', 'suffocate', 'punch',
        'fight', 'assault', 'harm', 'injure', 'wound', 'maim'
    ],
    'weapons_explosives': [
        'bomb', 'explosive', 'weapon', 'gun', 'knife', 'grenade', 'ammunition',
        'firearm', 'detonate', 'blast', 'missile', 'dynamite', 'c4', 'ied',
        'rifle', 'pistol', 'molotov'
    ],
    'illegal_drugs': [
        'drug', 'cocaine', 'heroin', 'meth', 'methamphetamine', 'narcotic',
        'trafficking', 'dealer', 'smuggle', 'cannabis', 'fentanyl', 'mdma',
        'ecstasy', 'lsd', 'marijuana', 'opioid', 'cartel'
    ],
    'child_harm': [
        'child', 'children', 'kid', 'minor', 'underage', 'baby', 'infant',
        'teenager', 'youth', 'juvenile', 'toddler', 'adolescent', 'schoolchild'
    ],
    'suicide_selfharm': [
        'suicide', 'self-harm', 'self harm', 'selfharm', 'kill yourself',
        'end your life', 'take your own life', 'cut yourself', 'hurt yourself'
    ],
    'theft_fraud': [
        'steal', 'theft', 'fraud', 'scam', 'embezzle', 'counterfeit', 'forge',
        'cheat', 'swindle', 'con', 'phishing', 'identity theft', 'credit card fraud',
        'insider trading', 'tax evasion', 'money laundering'
    ],
    'hate_discrimination': [
        'hate', 'discrimination', 'racist', 'racism', 'sexist', 'sexism',
        'homophobic', 'transphobic', 'antisemitic', 'islamophobic', 'bigot',
        'prejudice', 'slur', 'derogatory'
    ],
    'manipulation': [
        'manipulate', 'deceive', 'trick', 'coerce', 'blackmail', 'extort',
        'gaslight', 'brainwash', 'exploit', 'abuse'
    ],
    'illegal_general': [
        'illegal', 'unlawful', 'criminal', 'illicit', 'contraband', 'smuggle',
        'traffic', 'launder'
    ],
    'other': []
}

def categorize_prompt(prompt_text):
    """Categorize prompt into harm categories"""
    prompt_lower = prompt_text.lower()
    categories = []

    for category, keywords in harm_categories.items():
        if category == 'other':
            continue
        if any(keyword in prompt_lower for keyword in keywords):
            categories.append(category)

    if not categories:
        categories.append('other')

    return categories

# ============================================================================
# MAIN PROCESSING LOOP
# ============================================================================

for model_config in MODELS:

    model_name = model_config['name']
    display_name = model_config['display_name']
    file_name = model_config['file']

    print("\n" + "="*70)
    print(f"{display_name} - REFUSAL ANALYSIS DASHBOARD")
    print("="*70)

    # ========================================================================
    # STEP 1: LOAD DATA
    # ========================================================================

    print(f"\n[1/7] Loading {display_name} dataset...")

    file_path = f'{BASE_DIR}/{file_name}'
    df = pd.read_csv(file_path)

    print(f"✓ Loaded {len(df)} prompts")
    print(f"  Columns: {df.columns.tolist()}")
    print(f"  First few rows:")
    print(df.head(3))

    # Standardize column names
    if 'projection_last_layer' in df.columns:
        df = df.rename(columns={'projection_last_layer': 'refusal_score'})
    elif 'refusal_score' not in df.columns:
        # Try to find the score column
        score_cols = [col for col in df.columns if 'score' in col.lower() or 'projection' in col.lower()]
        if score_cols:
            df = df.rename(columns={score_cols[0]: 'refusal_score'})
            print(f"  Renamed '{score_cols[0]}' to 'refusal_score'")

    # Check if 'split' column exists, if not, create it or handle differently
    if 'split' not in df.columns:
        print(f"  WARNING: 'split' column not found. Checking for alternatives...")
        # Check if there's a column that indicates harmful/harmless
        if 'label' in df.columns:
            df = df.rename(columns={'label': 'split'})
            print(f"  Renamed 'label' to 'split'")
        elif 'type' in df.columns:
            df = df.rename(columns={'type': 'split'})
            print(f"  Renamed 'type' to 'split'")
        else:
            # If no split column exists, assume all are harmful
            print(f"  No split column found. Assuming all prompts are harmful.")
            df['split'] = 'harmful'

    # ========================================================================
    # STEP 2: SEPARATE HARMFUL/HARMLESS
    # ========================================================================

    print(f"\n[2/7] Separating harmful and harmless prompts...")

    df_harmful = df[df['split'] == 'harmful'].copy()
    df_harmless = df[df['split'] == 'harmless'].copy()

    print(f"  Harmful prompts: {len(df_harmful)}")
    print(f"  Harmless prompts: {len(df_harmless)}")

    # If no harmless prompts, we can't calculate Cohen's d properly
    if len(df_harmless) == 0:
        print(f"  WARNING: No harmless prompts found. Creating dummy harmless set for statistics.")
        # We'll skip the Cohen's d calculation or use a placeholder
        has_harmless = False
    else:
        has_harmless = True

    # ========================================================================
    # STEP 3: CALCULATE STATISTICS
    # ========================================================================

    print(f"\n[3/7] Calculating refusal direction statistics...")

    if has_harmless:
        harmful_mean = df_harmful['refusal_score'].mean()
        harmless_mean = df_harmless['refusal_score'].mean()
        harmful_std = df_harmful['refusal_score'].std()
        harmless_std = df_harmless['refusal_score'].std()

        # Cohen's d
        pooled_std = np.sqrt(((len(df_harmful) - 1) * harmful_std**2 +
                               (len(df_harmless) - 1) * harmless_std**2) /
                              (len(df_harmful) + len(df_harmless) - 2))
        cohens_d = (harmful_mean - harmless_mean) / pooled_std

        # Magnitude and separation
        magnitude = abs(harmful_mean - harmless_mean)
        separation = magnitude

        # Statistical test
        t_stat, p_value = stats.ttest_ind(df_harmful['refusal_score'],
                                           df_harmless['refusal_score'])
    else:
        # No harmless prompts - use placeholders
        cohens_d = np.nan
        magnitude = np.nan
        separation = np.nan
        p_value = np.nan

    print(f"\n  REFUSAL DIRECTION METRICS:")
    if has_harmless:
        print(f"  Cohen's d: {cohens_d:.3f}")
        print(f"  Magnitude: {magnitude:.3f}")
        print(f"  Separation: {separation:.3f}")
        print(f"  p-value: {p_value:.3e}")
    else:
        print(f"  Cohen's d: N/A (no harmless prompts)")
        print(f"  Magnitude: N/A")
        print(f"  Separation: N/A")
        print(f"  p-value: N/A")

    # ========================================================================
    # STEP 4: CATEGORIZE HARMFUL PROMPTS
    # ========================================================================

    print(f"\n[4/7] Categorizing harmful prompts...")

    df_harmful['categories'] = df_harmful['prompt'].apply(categorize_prompt)
    df_harmful['primary_category'] = df_harmful['categories'].apply(lambda x: x[0])

    category_counts = df_harmful['primary_category'].value_counts()
    category_scores = df_harmful.groupby('primary_category')['refusal_score'].agg(['mean', 'std', 'count'])
    category_scores = category_scores.sort_values('mean', ascending=False)

    print(f"\n  Categories identified: {len(category_counts)}")
    print(f"\n  Top 5 categories by mean score:")
    for idx, row in category_scores.head(5).iterrows():
        print(f"    {idx.replace('_', ' ').title()}: {row['mean']:.2f} (n={int(row['count'])})")

    # ========================================================================
    # STEP 5: SCORE STATISTICS
    # ========================================================================

    print(f"\n[5/7] Computing score statistics...")

    score_stats = {
        'Mean': df_harmful['refusal_score'].mean(),
        'Median': df_harmful['refusal_score'].median(),
        'Std Dev': df_harmful['refusal_score'].std(),
        'Min': df_harmful['refusal_score'].min(),
        'Max': df_harmful['refusal_score'].max(),
        'IQR': df_harmful['refusal_score'].quantile(0.75) - df_harmful['refusal_score'].quantile(0.25),
        'Q1': df_harmful['refusal_score'].quantile(0.25),
        'Q3': df_harmful['refusal_score'].quantile(0.75)
    }

    print("\n  SCORE STATISTICS:")
    for key, value in score_stats.items():
        print(f"    {key}: {value:.2f}")

    # Rank prompts
    df_harmful['refusal_rank'] = df_harmful['refusal_score'].rank(ascending=False, method='min')
    df_harmful = df_harmful.sort_values('refusal_rank')

    # ========================================================================
    # STEP 6: CREATE DASHBOARD
    # ========================================================================

    print(f"\n[6/7] Creating visualization dashboard...")

    # Set style
    plt.style.use('default')
    sns.set_palette("husl")

    # Create main figure
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)

    # ====================================================================
    # Plot 1: Score Distribution Histogram
    # ====================================================================
    ax1 = fig.add_subplot(gs[0, 0])

    ax1.hist(df_harmful['refusal_score'], bins=50, alpha=0.7, color='steelblue', edgecolor='black')
    ax1.axvline(score_stats['Mean'], color='red', linestyle='--', linewidth=2,
                label=f"Mean: {score_stats['Mean']:.2f}")
    ax1.axvline(score_stats['Median'], color='orange', linestyle='--', linewidth=2,
                label=f"Median: {score_stats['Median']:.2f}")

    ax1.set_xlabel('Refusal Score', fontsize=11, fontweight='bold')
    ax1.set_ylabel('Frequency', fontsize=11, fontweight='bold')
    ax1.set_title('Score Distribution', fontsize=12, fontweight='bold')
    ax1.legend(fontsize=9)
    ax1.grid(True, alpha=0.3, axis='y')

    # ====================================================================
    # Plot 2: Model Info Box
    # ====================================================================
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.axis('off')

    if has_harmless:
        refusal_info = f"""• Cohen's d: {cohens_d:.3f}
• Magnitude: {magnitude:.3f}
• Separation: {separation:.3f}
• p-value: {p_value:.2e}"""
    else:
        refusal_info = """• Cohen's d: N/A
• Magnitude: N/A
• Separation: N/A
• p-value: N/A"""

    info_text = f"""MODEL: {display_name}

ARCHITECTURE:
• Layers: {model_config['layers']}
• Hidden Size: {model_config['hidden_size']}

REFUSAL DIRECTION:
{refusal_info}

SCORE STATISTICS:
• Mean: {score_stats['Mean']:.3f}
• Median: {score_stats['Median']:.3f}
• Std Dev: {score_stats['Std Dev']:.3f}
• Range: [{score_stats['Min']:.2f}, {score_stats['Max']:.2f}]
• IQR: [{score_stats['Q1']:.2f}, {score_stats['Q3']:.2f}]

DATASET:
• Total Prompts: {len(df_harmful)}
• Categories: {len(category_counts)}"""

    ax2.text(0.1, 0.5, info_text, fontsize=10, family='monospace',
             verticalalignment='center',
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))

    # ====================================================================
    # Plot 3: Harm Category Distribution (Pie Chart)
    # ====================================================================
    ax3 = fig.add_subplot(gs[0, 2])

    category_percentages = (category_counts / category_counts.sum()) * 100
    colors = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))

    # Create pie chart with legend instead of labels
    wedges, texts, autotexts = ax3.pie(category_counts,
                                         autopct='%1.1f%%',
                                         colors=colors,
                                         startangle=90,
                                         pctdistance=0.85)

    for autotext in autotexts:
        autotext.set_color('black')
        autotext.set_fontsize(9)
        autotext.set_fontweight('bold')

    # Add legend outside the pie chart
    ax3.legend(wedges, [c.replace('_', ' ').title() for c in category_counts.index],
              loc="center left",
              bbox_to_anchor=(1, 0, 0.5, 1),
              fontsize=9)

    ax3.set_title('Harm Category Distribution', fontsize=12, fontweight='bold')

    # ====================================================================
    # Plot 4: Top 10 Categories by Mean Score
    # ====================================================================
    ax4 = fig.add_subplot(gs[1, :])

    top_10_cats = category_scores.head(10)
    x_pos = np.arange(len(top_10_cats))
    colors_bar = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(top_10_cats)))

    bars = ax4.bar(x_pos, top_10_cats['mean'], color=colors_bar, alpha=0.8, edgecolor='black')

    # Add count labels
    for i, (bar, count) in enumerate(zip(bars, top_10_cats['count'])):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                f'n={int(count)}',
                ha='center', va='bottom', fontsize=9, fontweight='bold')

    ax4.axhline(score_stats['Mean'], color='red', linestyle='--', linewidth=2,
               label=f"Overall Mean: {score_stats['Mean']:.2f}", alpha=0.7)

    ax4.set_xlabel('Category', fontsize=11, fontweight='bold')
    ax4.set_ylabel('Mean Refusal Score', fontsize=11, fontweight='bold')
    ax4.set_title('Top 10 Categories by Mean Refusal Score', fontsize=12, fontweight='bold')
    ax4.set_xticks(x_pos)
    ax4.set_xticklabels([c.replace('_', '\n').title() for c in top_10_cats.index], fontsize=9)
    ax4.legend(fontsize=9)
    ax4.grid(True, alpha=0.3, axis='y')

    # ====================================================================
    # Plot 5: Box Plot
    # ====================================================================
    ax5 = fig.add_subplot(gs[2, 0])

    top_5_cats = category_scores.head(5).index.tolist()
    data_for_box = [df_harmful[df_harmful['primary_category'] == cat]['refusal_score'].values
                    for cat in top_5_cats]

    bp = ax5.boxplot(data_for_box,
                     labels=[c.replace('_', '\n').title() for c in top_5_cats],
                     patch_artist=True,
                     showmeans=True)

    for patch, color in zip(bp['boxes'], plt.cm.Set2(np.linspace(0, 1, 5))):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax5.set_ylabel('Refusal Score', fontsize=11, fontweight='bold')
    ax5.set_title('Score Distribution by Top 5 Categories', fontsize=12, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')
    plt.setp(ax5.get_xticklabels(), fontsize=9)

    # ====================================================================
    # Plot 6: Percentiles
    # ====================================================================
    ax6 = fig.add_subplot(gs[2, 1])

    percentiles = [10, 20, 30, 40, 50, 60, 70, 80, 90]
    percentile_scores = [df_harmful['refusal_score'].quantile(p/100) for p in percentiles]

    ax6.plot(percentiles, percentile_scores, 'o-', linewidth=3, markersize=8, color='darkblue')
    ax6.fill_between(percentiles, percentile_scores, alpha=0.3, color='lightblue')

    ax6.set_xlabel('Percentile', fontsize=11, fontweight='bold')
    ax6.set_ylabel('Refusal Score', fontsize=11, fontweight='bold')
    ax6.set_title('Score Percentiles', fontsize=12, fontweight='bold')
    ax6.grid(True, alpha=0.3)

    # Annotate median
    median_val = score_stats['Median']
    ax6.annotate(f'{median_val:.1f}',
                xy=(50, median_val),
                xytext=(50, median_val+5),
                fontsize=9, ha='center', fontweight='bold',
                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))

    # ====================================================================
    # Plot 7: Cumulative Distribution
    # ====================================================================
    ax7 = fig.add_subplot(gs[2, 2])

    sorted_scores = np.sort(df_harmful['refusal_score'])
    cumulative = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores) * 100

    ax7.plot(sorted_scores, cumulative, linewidth=2.5, color='darkgreen')
    ax7.fill_between(sorted_scores, cumulative, alpha=0.3, color='lightgreen')

    ax7.axvline(score_stats['Median'], color='red', linestyle='--', linewidth=2,
               label=f'Median: {score_stats["Median"]:.2f}')
    ax7.axhline(50, color='red', linestyle='--', linewidth=1, alpha=0.5)

    ax7.set_xlabel('Refusal Score', fontsize=11, fontweight='bold')
    ax7.set_ylabel('Cumulative %', fontsize=11, fontweight='bold')
    ax7.set_title('Cumulative Distribution', fontsize=12, fontweight='bold')
    ax7.legend(fontsize=9)
    ax7.grid(True, alpha=0.3)

    # ====================================================================
    # Save Dashboard
    # ====================================================================
    plt.suptitle(f'{display_name} - Refusal Analysis Dashboard',
                fontsize=18, fontweight='bold', y=0.998)

    fig_path = f'{BASE_DIR}/{model_name}_dashboard.png'
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"✓ Dashboard saved: {fig_path}")

    plt.close()

    # ========================================================================
    # STEP 7: CREATE PROMPTS TABLE
    # ========================================================================

    print(f"\n[7/7] Creating prompts table...")

    fig2, axes = plt.subplots(2, 1, figsize=(18, 12))

    # Top 10
    ax_top = axes[0]
    ax_top.axis('tight')
    ax_top.axis('off')

    top_10 = df_harmful.head(10)

    table_data_top = []
    for idx, row in top_10.iterrows():
        rank = int(row['refusal_rank'])
        score = f"{row['refusal_score']:.3f}"
        categories = ', '.join(row['categories'][:2])
        prompt = row['prompt'][:85] + "..." if len(row['prompt']) > 85 else row['prompt']
        table_data_top.append([rank, score, categories, prompt])

    table_top = ax_top.table(cellText=table_data_top,
                            colLabels=['Rank', 'Score', 'Categories', 'Prompt'],
                            cellLoc='left',
                            colWidths=[0.06, 0.08, 0.18, 0.68],
                            loc='center',
                            bbox=[0, 0, 1, 1])

    table_top.auto_set_font_size(False)
    table_top.set_fontsize(10)
    table_top.scale(1, 2.5)

    for i in range(len(table_data_top)):
        color = plt.cm.Reds(0.3 + (i / len(table_data_top)) * 0.5)
        for j in range(4):
            table_top[(i+1, j)].set_facecolor(color)
            table_top[(i+1, j)].set_alpha(0.3)

    for j in range(4):
        table_top[(0, j)].set_facecolor('darkblue')
        table_top[(0, j)].set_text_props(weight='bold', color='white')

    ax_top.set_title(f'Top 10 Highest Refusal Score Prompts ({display_name})',
                    fontsize=14, fontweight='bold', pad=20)

    # Bottom 10
    ax_bottom = axes[1]
    ax_bottom.axis('tight')
    ax_bottom.axis('off')

    bottom_10 = df_harmful.tail(10)

    table_data_bottom = []
    for idx, row in bottom_10.iterrows():
        rank = int(row['refusal_rank'])
        score = f"{row['refusal_score']:.3f}"
        categories = ', '.join(row['categories'][:2])
        prompt = row['prompt'][:85] + "..." if len(row['prompt']) > 85 else row['prompt']
        table_data_bottom.append([rank, score, categories, prompt])

    table_bottom = ax_bottom.table(cellText=table_data_bottom,
                                   colLabels=['Rank', 'Score', 'Categories', 'Prompt'],
                                   cellLoc='left',
                                   colWidths=[0.06, 0.08, 0.18, 0.68],
                                   loc='center',
                                   bbox=[0, 0, 1, 1])

    table_bottom.auto_set_font_size(False)
    table_bottom.set_fontsize(10)
    table_bottom.scale(1, 2.5)

    for i in range(len(table_data_bottom)):
        color = plt.cm.Greens(0.3 + (i / len(table_data_bottom)) * 0.4)
        for j in range(4):
            table_bottom[(i+1, j)].set_facecolor(color)
            table_bottom[(i+1, j)].set_alpha(0.3)

    for j in range(4):
        table_bottom[(0, j)].set_facecolor('darkgreen')
        table_bottom[(0, j)].set_text_props(weight='bold', color='white')

    ax_bottom.set_title(f'Top 10 Lowest Refusal Score Prompts ({display_name})',
                       fontsize=14, fontweight='bold', pad=20)

    plt.tight_layout()
    table_path = f'{BASE_DIR}/{model_name}_prompts_table.png'
    plt.savefig(table_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"✓ Prompts table saved: {table_path}")

    plt.close()

    # ========================================================================
    # SAVE CSV RESULTS
    # ========================================================================

    # Ranked prompts
    df_harmful_export = df_harmful[['refusal_rank', 'refusal_score', 'primary_category', 'categories', 'prompt']].copy()
    df_harmful_export['categories'] = df_harmful_export['categories'].apply(lambda x: ', '.join(x))
    df_harmful_export.to_csv(f'{BASE_DIR}/{model_name}_ranked_prompts.csv', index=False)

    # Category stats
    category_scores.to_csv(f'{BASE_DIR}/{model_name}_category_stats.csv')

    # Summary stats
    summary_stats = pd.DataFrame({
        'Metric': ['Mean', 'Median', 'Std Dev', 'Min', 'Max', 'IQR', 'Q1', 'Q3',
                   'Cohens_d', 'Magnitude', 'Separation', 'P_value'],
        'Value': [score_stats['Mean'], score_stats['Median'], score_stats['Std Dev'],
                  score_stats['Min'], score_stats['Max'], score_stats['IQR'],
                  score_stats['Q1'], score_stats['Q3'],
                  cohens_d if has_harmless else np.nan,
                  magnitude if has_harmless else np.nan,
                  separation if has_harmless else np.nan,
                  p_value if has_harmless else np.nan]
    })
    summary_stats.to_csv(f'{BASE_DIR}/{model_name}_summary_stats.csv', index=False)

    print("\n" + "="*70)
    print(f"✓ {display_name} ANALYSIS COMPLETE!")
    print("="*70)
    print(f"\nKey Findings:")
    print(f"  • Mean Refusal Score: {score_stats['Mean']:.2f}")
    print(f"  • Median Refusal Score: {score_stats['Median']:.2f}")
    if has_harmless:
        print(f"  • Cohen's d: {cohens_d:.3f}")
    else:
        print(f"  • Cohen's d: N/A (no harmless prompts)")
    print(f"  • Top Category: {category_scores.index[0].replace('_', ' ').title()}")
    print(f"  • Total Harmful Prompts: {len(df_harmful)}")
    print("="*70)

print("\n\n" + "="*70)
print("✓✓✓ ALL MODELS PROCESSED SUCCESSFULLY ✓✓✓")
print("="*70)

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/FedericoPierucci/LLM-refusal-directions-and-preferences
# %cd YOUR_REPO